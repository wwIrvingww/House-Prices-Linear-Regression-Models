---
title: "Logistic_regression"
author: "Irving, Chuy"
date: "2025-04-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(car)
library(reshape2)
library(ggplot2)
library(forcats)
library(glmnet)
library(caret)
library(profvis)
library(nnet) 
library(utils)
library(purrr)
```

2.Use los mismos conjuntos de entrenamiento y prueba que utilizó en las hojas anteriores.
```{r load_data, echo=FALSE}
#train data ya tiene el 70 y test_data el 30%
train_data <- read.csv("../train_final.csv")
test_data <- read.csv("../test_final.csv")
```
Se  cargó la data que se ha usado en entregas anteriores.

```{r remove_columns, echo=FALSE}
train_data <- train_data %>% select(-Condition2, -RoofMatl, -Exterior2nd, -Electrical)
test_data <- test_data %>% select(-Condition2, -RoofMatl, -Exterior2nd, -Electrical)
```
Se removieron las columnas problematicas

```{r remove_na_columns, echo=FALSE}
pct_na <- sapply(train_data, function(x) mean(is.na(x))*100)
drop_cols <- names(pct_na[pct_na > 75])
train_data <- train_data %>% select(-all_of(drop_cols))
test_data  <- test_data  %>% select(-all_of(drop_cols))
```
Se remueven las columnas que no dan información útil.

```{r discretization, echo=FALSE}
breaks <- quantile(train_data$SalePrice, probs = c(0,1/3,2/3,1), na.rm=TRUE)
labels <- c("barata","estandar","cara")

train_data$CategoriaPrecio <- cut(train_data$SalePrice, breaks=breaks, labels=labels, include.lowest=TRUE)
test_data$CategoriaPrecio  <- cut(test_data$SalePrice,  breaks=breaks, labels=labels, include.lowest=TRUE)

```
Creamos las tres categorías de las casas (barata, estándar, cara.)

1. Cree una variable dicotómica por cada una de las categorías de la variable respuesta categórica que creó en hojas anteriores. Debería tener 3 variables dicotómicas (valores 0 y 1) una que diga si la vivienda es cara o no, media o no, económica o no.
```{r dicotomic_variable, echo=TRUE}
# Para el conjunto de entrenamiento
train_data <- train_data %>%
  mutate(
    Cara = ifelse(CategoriaPrecio == "cara", 1, 0),
    Estandar = ifelse(CategoriaPrecio == "estandar", 1, 0),
    Economica = ifelse(CategoriaPrecio == "barata", 1, 0)
  )

# Para el conjunto de prueba
test_data <- test_data %>%
  mutate(
    Cara = ifelse(CategoriaPrecio == "cara", 1, 0),
    Estandar = ifelse(CategoriaPrecio == "estandar", 1, 0),
    Economica = ifelse(CategoriaPrecio == "barata", 1, 0)
  )
```
Se crearon las tres variables dicotómicas para las categorías `barata`, `estandar` y `cara`

3. Elabore un modelo de regresión logística para conocer si una vivienda es cara o no, utilizando el conjunto
de entrenamiento y explique los resultados a los que llega. El experimento debe ser reproducible por lo
que debe fijar que los conjuntos de entrenamiento y prueba sean los mismos siempre que se ejecute el
código. Use validación cruzada.

```{r process_data, echo=FALSE}
numeric_cols <- train_data %>% select(where(is.numeric)) %>% names()

all_cat <- train_data %>% select(where(~!is.numeric(.))) %>% names()
valid_cat <- all_cat[
  sapply(all_cat, function(col) {
    n_distinct(train_data[[col]]) > 1 && n_distinct(test_data[[col]]) > 1
  })
]

preNum <- preProcess(train_data[numeric_cols], method = "medianImpute")
train_data[numeric_cols] <- predict(preNum, train_data[numeric_cols])
test_data[numeric_cols]  <- predict(preNum, test_data[numeric_cols])

encoder <- dummyVars(~ ., data = train_data[valid_cat], fullRank = TRUE)
train_cat <- predict(encoder, train_data[valid_cat]) %>% as.data.frame()
test_cat  <- predict(encoder, test_data[valid_cat]) %>% as.data.frame()


missing <- setdiff(names(train_cat), names(test_cat))
if(length(missing) > 0){
  test_cat[missing] <- 0
  test_cat <- test_cat[names(train_cat)]
}

train_encoded <- bind_cols(train_data[numeric_cols], train_cat, 
                           CategoriaPrecio = train_data$CategoriaPrecio)
test_encoded  <- bind_cols(test_data[numeric_cols],  test_cat,  
                           CategoriaPrecio = test_data$CategoriaPrecio)

```
Antes de elaborar el modelo, se realizó un procesamiento de la data para que sea compatible con el modelo que vamos a utilizar. Ya que el modelo no puede contener NAN's y el número de observaciones que contenía NAN's era menor a 100, se optó por omitir esas filas.



```{r logistic_regression, echo=FALSE}
set.seed(1234)

model_data <- train_data %>% select(-SalePrice, -CategoriaPrecio, -Estandar, -Economica)

model_data <- na.omit(model_data)

train_control <- trainControl(method = "cv", number = 10)

logistic_model <- train(as.factor(Cara) ~ ., 
                        data = model_data, 
                        method = "glm", 
                        family = "binomial", 
                        trControl = train_control)
print(logistic_model)


```
El primer aspecto a destacar es el aviso proporcionado por la librería: "glm.fit: algorithm did not converge". Esto indica que el algoritmo iterativo utilizado para ajustar la regresión logística no logró alcanzar una convergencia completa. En otras palabras, después de varias iteraciones, el método no pudo encontrar un conjunto de parámetros estables. Esta situación puede ocurrir cuando hay variables altamente correlacionadas (multicolinealidad) o cuando se presenta una separación completa o casi completa de las clases en algunos predictores.

Otro aviso relevante es "prediction from rank-deficient fit; attr(*, 'non-estim') has doubtful cases". Este mensaje sugiere que el modelo es deficiente en rango lo que significa que existen redundancias entre las variables predictoras, impidiendo que algunos coeficientes se estimen de manera única. Esto suele suceder cuando hay variables con alta correlación o cuando se incluyen más predictores de los que la información en la muestra puede soportar.

En cuanto a las estadísticas obtenidas, se alcanzó una `accuracy` de 0.7345955 y un `kappa` de 0.4682196, lo que sugiere que, a pesar de los avisos, el modelo presenta un desempeño moderado al clasificar las viviendas como "caras" o "no caras". Aunque el rendimiento predictivo es aceptable, los avisos indican la necesidad de ser cauteloso al interpretar los coeficientes y confiar en la estabilidad del modelo. Por lo tanto, se procederá a realizar un análisis más detallado para investigar la posible existencia de multicolinealidad.

4. Analice el modelo. Determine si hay multicolinealidad en las variables, y cuáles son las que aportan al
modelo, por su valor de significación. Haga un análisis de correlación de las variables del modelo y
especifique si el modelo se adapta bien a los datos.

```{r modelo_summary, echo=TRUE}
full_model <- glm(as.factor(Cara) ~ ., data = model_data, family = "binomial")
summary(full_model)
```

```{r predictors_non_alias, echo=TRUE, warning=FALSE, message=FALSE}
coef_names <- names(coef(full_model))

aliased <- alias(full_model)$Complete
non_alias <- coef_names[!coef_names %in% rownames(aliased)]

predictors_non_alias <- non_alias[non_alias != "(Intercept)"]

print("Predictors no aliasados:")
print(predictors_non_alias)
```

```{r settings_model, echo=TRUE}
available_predictors <- predictors_non_alias[predictors_non_alias %in% names(model_data)]
print("Predictores disponibles en model_data:")
print(available_predictors)

available_predictors_backticks <- paste0("`", available_predictors, "`")

fmla_non_alias <- as.formula(paste("as.factor(Cara) ~", 
                                   paste(available_predictors_backticks, collapse = " + ")))
print("Fórmula del modelo sin predictores aliasados y con variables disponibles:")
print(fmla_non_alias)

full_model_non_alias <- glm(fmla_non_alias, data = model_data, family = "binomial")

summary(full_model_non_alias)
```

```{r vif_stimate, echo=TRUE}

aliased <- alias(full_model)$Complete
print("Variables colineales (alias):")
print(aliased)

coef_names <- names(coef(full_model))

non_alias <- coef_names[!coef_names %in% rownames(aliased)]
print("Coeficientes no aliasados:")
print(non_alias)

available_predictors <- predictors_non_alias[predictors_non_alias %in% names(model_data)]
print("Predictors no alias presentes en model_data:")
print(available_predictors)

available_predictors_backticks <- paste0("`", available_predictors, "`")
fmla_non_alias <- as.formula(paste("as.factor(Cara) ~", 
                                   paste(available_predictors_backticks, collapse = " + ")))
print("Fórmula del modelo sin predictores aliasados y solo con variables disponibles:")
print(fmla_non_alias)

full_model_non_alias <- glm(fmla_non_alias, data = model_data, family = "binomial")

vif_values <- vif(full_model_non_alias)
print("Valores VIF para el modelo sin coeficientes aliasados:")
print(vif_values)

```

Dentro de la función VIF se calcularon distintos valores de los cuales se pueden discutir los siguientes aspectos: variables como `Id` con un VIF de 1.16, `MSSubClass` con 1.59, `LotFrontage` con 1.66 y `LotArea` con 1.41 presentan coeficientes entre 1 y 3, lo que indica poca o ninguna colinealidad significativa en esos casos. Sin embargo, se observan valores moderadamente altos, como `X1stFlrSF` con 7.39 y `X2ndFlrSF` con 8.15, que sugieren una correlación con otras variables. 

Por otro lado, hay variables que presentan valores muy altos superiores a 10 como `YearBuilt` con aproximadamente 10.65, `GarageYrBlt` con 10.06, `BsmtFinSF1` con 9.58 y `BsmtUnfSF` con 8.62. Estos valores indican un problema de multicolinealidad fuerte en estos predictores. No obstante esta alta multicolinealidad puede ser razonable en ciertos casos, como la relación entre el año de construcción y el año de construcción del garaje, así como la alta correlación entre las medidas de superficie del sótano y la superficie del primer piso.

En general aunque muchas de las variables tienen VIF razonablemente bajos, la presencia de algunos predictores con VIF superiores a 10 (o cercanos a esos valores) es una señal de redundancia en la información. Esto podría hacer que las estimaciones de los coeficientes sean inestables y dificultar la interpretación individual de estos.


```{r correlacion_predictors_fixed, echo=TRUE, warning=FALSE, message=FALSE}
predictors_numeric <- model_data %>% 
  select(-Cara) %>% 
  select(where(is.numeric))

corr_matrix <- cor(predictors_numeric, use = "complete.obs")
print("Matriz de correlación (redondeada):")
print(round(corr_matrix, 2))
```

Al analizar la matriz de correlación se observa que algunas variables categóricas como el tipo de construcción (`MSSubClass`) presentan correlaciones moderadas con otras medidas. Por ejemplo, hay una correlación negativa moderada de aproximadamente –0.32 entre `MSSubClass` y `LotFrontage`, sugiriendo diferencias en el frente de lote según la categoría estructural de las viviendas.

Las variables relacionadas con la calidad y condición de la vivienda también destacan. En particular “OverallQual” muestra correlaciones positivas significativas con “YearBuilt” (0.53) y “YearRemodAdd” (0.57), indicando que las viviendas más modernas o recientemente remodeladas tienden a tener calidades percibidas más altas.

En cuanto a las medidas de área, “TotalBsmtSF” y “X1stFlrSF” tienen una correlación muy alta (cercana a 0.90), sugiriendo que ambas miden aspectos del tamaño de la vivienda y presentan redundancia contribuyendo a la multicolinealidad del modelo. Otros indicadores de área, como “LotFrontage”, “LotArea” y “GrLivArea”, también muestran correlaciones moderadas entre sí.

Las variables temporales, “YearBuilt” y “YearRemodAdd”, tienen una correlación alta (0.69), indicando que en muchas viviendas el año de construcción y el de remodelación están estrechamente relacionados.

Aunque muchas variables presentan correlaciones moderadas o bajas existen grupos de variable especialmente relacionadas con el tamaño y la estructura de la vivienda que están fuertemente correlacionadas. Esto refuerza la preocupación por la multicolinealidad, como se evidenció en los altos valores de VIF. La matriz de correlación sugiere que una reducción de dimensiones o una selección cuidadosa de predictores podría mejorar la interpretabilidad del modelo sin comprometer su capacidad predictiva.

```{r heatmap_correlacion, echo=FALSE, warning=FALSE, message=FALSE}
melted_corr <- melt(corr_matrix)

ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlación") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.title = element_blank()) +
  labs(title = "Heatmap: Matriz de correlación de los predictores")
```
El heatmap de la matriz de correlación confirma de manera visual los hallazgos numéricos previos. Las áreas más intensamente coloreadas en rojo indican pares de variables con correlaciones positivas altas, mientras que los bloques en azul reflejan correlaciones negativas notables. En particular puede apreciarse un clúster de variables que representan la dimensión de la vivienda (por ejemplo, TotalBsmtSF, X1stFlrSF, GarageCars, GarageArea) mostrando altas correlaciones entre sí, lo que respalda la existencia de multicolinealidad detectada por los valores de VIF. De forma similar, otras zonas del mapa exhiben correlaciones elevadas entre las variables temporales (YearBuilt, YearRemodAdd, GarageYrBlt) y los indicadores de calidad (OverallQual, OverallCond). 

5. Utilice el modelo con el conjunto de prueba y determine la eficiencia del algoritmo para clasificar.  
```{r model_test, echo=FALSE}
# Predecir las probabilidades para el conjunto de prueba usando el modelo ajustado (full_model_non_alias)
pred_probs <- predict(full_model_non_alias, newdata = test_data, type = "response")

# Convertir las probabilidades en clases (usamos un umbral de 0.5)
pred_class <- ifelse(pred_probs >= 0.5, 1, 0)

# Asegurarse que tanto las predicciones como la variable real de respuesta sean factores
pred_class <- as.factor(pred_class)
actual_class <- as.factor(test_data$Cara)

conf_mat <- confusionMatrix(pred_class, actual_class)
print(conf_mat)

```
La matriz de confusión y las estadísticas resultantes indican que el modelo de clasificación binaria (para predecir si una vivienda es "cara" o no) tiene un desempeño robusto. Con una **exactitud del 86.33%** y un intervalo de confianza del 95% entre 82.76% y 89.41%, el modelo supera significativamente la tasa de no información (66.06%), lo que se refleja en un p-valor muy bajo (< 2.2e-16) al comparar con la tasa de no información. Además, el valor de **Kappa (0.6806)** sugiere un acuerdo sustancial entre las predicciones y las observaciones reales, más allá de lo que se esperaría por azar.

En detalle, la **sensibilidad del 94.48%** (dado que la clase positiva se define como la etiqueta 0) indica que el modelo identifica correctamente la gran mayoría de las viviendas que pertenecen a la clase positiva. La **especificidad del 70.47%** evidencia que, aunque la identificación de la clase contraria (viviendas "cara", en este contexto) es moderada, el desempeño en general es bastante equilibrado, como lo refleja la **exactitud balanceada del 82.48%**. Los valores altos de las métricas de valor predictivo (Pos Pred Value: 86.16% y Neg Pred Value: 86.78%) refuerzan la idea de que el modelo es confiable al asignar correctamente la clase a nuevas observaciones.

Finalmente, el resultado del Test de McNemar (p-value = 0.0004909) indica que las discrepancias en los errores de clasificación son estadísticamente significativas, lo que añade evidencia a la robustez del modelo. En conjunto, estos resultados respaldan que el modelo tiene una capacidad predictiva sólida para discriminar entre viviendas clasificadas como "cara" y "no cara".


6. Explique si hay sobreajuste (overfitting) o no (recuerde usar para esto los errores del conjunto de prueba
y de entrenamiento). Muestre las curvas de aprendizaje usando los errores de los conjuntos de
entrenamiento y prueba
```{r prepare_data, echo=FALSE}
prep_data <- function(data) {
  # Eliminar columnas no relevantes y con muchos NA
  data <- data %>%
    select(-any_of(c("SalePrice", "CategoriaPrecio", "Estandar", "Economica"))) %>%
    select(-where(~mean(is.na(.)) > 0.3)) %>%
    mutate(across(where(is.character), as.factor))
  
  # Eliminar filas con NA en la respuesta
  data <- data %>% filter(!is.na(Cara))
  
  return(data)
}

train_prep <- prep_data(train_data)
test_prep <- prep_data(test_data)

select_numeric_vars <- function(train_data, test_data, n_vars = 7) {
  numeric_vars <- train_data %>%
    select(where(is.numeric), -Cara) %>%
    names()

  cor_values <- sapply(numeric_vars, function(x) {
    cor(train_data[[x]], as.numeric(train_data$Cara), use = "complete.obs")
  })

  selected_num <- names(sort(abs(cor_values), decreasing = TRUE)[1:n_vars])

  cor_test <- sapply(selected_num, function(x) {
    cor(test_data[[x]], as.numeric(test_data$Cara), use = "complete.obs")
  })

  keep_num <- selected_num[sign(cor_values[selected_num]) == sign(cor_test)]
  
  return(keep_num)
}

numeric_selected <- select_numeric_vars(train_prep, test_prep, 7)
message("Variables numéricas seleccionadas: ", paste(numeric_selected, collapse = ", "))

select_categorical_vars <- function(train_data, test_data, n_vars = 7) {
  cat_vars <- train_data %>%
    select(where(is.factor), -Cara) %>%
    names()

  chi2_results <- sapply(cat_vars, function(x) {
    tbl <- table(train_data[[x]], train_data$Cara)
    if (any(dim(tbl) < 2)) return(0)
    test <- chisq.test(tbl, simulate.p.value = TRUE)
    -log10(test$p.value)
  })

  selected_cat <- names(sort(chi2_results, decreasing = TRUE)[1:n_vars])
  
  chi2_test <- sapply(selected_cat, function(x) {
    tbl <- table(test_data[[x]], test_data$Cara)
    if (any(dim(tbl) < 2)) return(0)
    test <- chisq.test(tbl, simulate.p.value = TRUE)
    -log10(test$p.value)
  })

  keep_cat <- selected_cat[chi2_test > 2] # p < 0.01
  
  return(keep_cat)
}

categorical_selected <- select_categorical_vars(train_prep, test_prep, 7)
message("Variables categóricas seleccionadas: ", paste(categorical_selected, collapse = ", "))


validate_with_rf <- function(train_data, num_vars, cat_vars) {
  if (!require("randomForest")) install.packages("randomForest")
  library(randomForest)
  
  model_data <- train_data %>%
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(across(all_of(cat_vars), as.factor),
           Cara = as.factor(Cara))  
  

  set.seed(123)
  rf_model <- randomForest(Cara ~ ., data = model_data, importance = TRUE)
  
  imp_df <- as.data.frame(importance(rf_model)) %>%
    mutate(Variable = rownames(.)) %>%
    arrange(desc(MeanDecreaseAccuracy))
  
  return(list(model = rf_model, importance = imp_df))
}

rf_result <- validate_with_rf(train_prep, numeric_selected, categorical_selected)


print(rf_result$importance)

create_final_datasets <- function(train_data, test_data, num_vars, cat_vars) {
  
  final_train <- train_data %>%
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(across(all_of(cat_vars), as.factor))
  
  final_test <- test_data %>%
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(across(all_of(cat_vars), as.factor))
  
  for (var in cat_vars) {
    if (nlevels(final_train[[var]]) < 2) {
      final_train <- final_train %>% select(-all_of(var))
      final_test <- final_test %>% select(-all_of(var))
      message("Variable categórica ", var, " eliminada por tener un solo nivel")
    }
  }
  
  return(list(
    train = final_train,
    test = final_test,
    numeric_vars = num_vars,
    categorical_vars = intersect(cat_vars, names(final_train))
  ))
}


select_final_vars <- function(importance_df, num_vars, cat_vars, n_final = 14) {
  final_vars <- importance_df$Variable[1:min(n_final, nrow(importance_df))]
  
  final_num <- intersect(final_vars, num_vars)
  final_cat <- intersect(final_vars, cat_vars)
  
  remaining <- n_final - length(final_num) - length(final_cat)
  if (remaining > 0) {

    all_vars <- c(num_vars, cat_vars)
    remaining_vars <- setdiff(all_vars, c(final_num, final_cat))
    to_add <- importance_df$Variable %>%
      intersect(remaining_vars) %>%
      head(remaining)
    
    final_num <- c(final_num, intersect(to_add, num_vars))
    final_cat <- c(final_cat, intersect(to_add, cat_vars))
  }
  
  return(list(num = final_num, cat = final_cat))
}

final_vars <- select_final_vars(rf_result$importance, numeric_selected, categorical_selected)


final_data <- create_final_datasets(
  train_prep, test_prep, 
  final_vars$num, final_vars$cat
)


varImpPlot(rf_result$model, main = "Importancia de Variables Seleccionadas")

message("\n=== VARIABLES FINALES SELECCIONADAS ===")
message("Numéricas (", length(final_data$numeric_vars), "):")
message(paste(final_data$numeric_vars, collapse = ", "))

message("\nCategóricas (", length(final_data$categorical_vars), "):")
message(paste(final_data$categorical_vars, collapse = ", "))

message("\nTop 5 variables más importantes:")
print(head(rf_result$importance, 5))


selected_vars <- list(
  numeric = final_data$numeric_vars,
  categorical = final_data$categorical_vars,
  importance = rf_result$importance
)


message("\nVerificación de niveles categóricos:")
for (var in final_data$categorical_vars) {
  message(var, ": ", nlevels(final_data$train[[var]]), " niveles")
}

message("\nDistribución de clases:")
table(final_data$train$Cara)
```
Se realiza la preparación y limpieza de datos eliminando columnas con demasiados valores perdidos (>30%), descartando variables no relevantes (`SalePrice`, `CategoriaPrecio`, etc.) y transformando a factores aquellas columnas de tipo carácter. Seguidamente, se implementan dos funciones de selección de variables: (1) `select_numeric_vars` para escoger las variables numéricas más correlacionadas con la variable respuesta `Cara` en ambos conjuntos (entrenamiento y prueba), y (2) `select_categorical_vars` para filtrar las variables categóricas con mayor relevancia estadística (p-valor bajo en test de Chi-cuadrado). Estas selecciones permiten reducir dimensionalidad y enfocarse en los predictores potencialmente más influyentes.

Tras ello se entrena un modelo de Random Forest (`validate_with_rf`) con las variables filtradas, obteniendo una tabla de importancia de variables (`importance`) que determina cuáles predictores aportan más a la clasificación. En particular, se ve que variables como `GrLivArea`, `X1stFlrSF`, `Neighborhood` y `OverallQual` destacan por su contribución, lo que coincide con la intuición de que tanto la superficie habitable como la calidad y la ubicación de la vivienda son determinantes relevantes para el costo o el tipo de la casa.

Finalmente con la función `create_final_datasets` se construyen los data frames finales para entrenamiento y prueba, asegurando que las columnas categóricas tengan niveles adecuados y que no existan variables categóricas con un solo nivel (que no aportarían información). El reporte de variables finales (7 numéricas y 5 categóricas) y los niveles de cada categoría** confirma que los datos están listos para el modelado, mientras que la **distribución de clases (687 para la clase 0 y 334 para la clase 1) sugiere un ligero desbalance pero no excesivo. 
```{r, echo=FALSE}

prepare_final_data <- function(data, num_vars, cat_vars) {

  data <- data %>% 
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(
      across(all_of(cat_vars), ~ forcats::fct_lump_prop(as.factor(.), prop = 0.05, other_level = "OTROS")),
      Cara = as.factor(Cara)
    )
  return(data)
}

train_final <- prepare_final_data(train_data, final_vars$numeric, final_vars$categorical)
test_final  <- prepare_final_data(test_data,  final_vars$numeric, final_vars$categorical)

for(var in final_vars$categorical){
  test_final[[var]] <- factor(test_final[[var]], levels = levels(train_final[[var]]))
}

message("Niveles en Neighborhood (train): ", paste(levels(train_final$Neighborhood), collapse = ", "))
message("Niveles en Neighborhood (test): ", paste(levels(test_final$Neighborhood), collapse = ", "))

```

```{r stadistics, echo=FALSE}

final_vars <- list(
  numeric = c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
              "GarageArea", "FullBath", "GarageCars"),
  categorical = c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")
)

prepare_final_data <- function(data, num_vars, cat_vars) {
  data %>% 
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(
      across(all_of(cat_vars), ~ forcats::fct_lump_prop(as.factor(.), prop = 0.05, other_level = "OTROS")),
      Cara = as.factor(Cara)
    )
}

train_final <- prepare_final_data(train_data, final_vars$numeric, final_vars$categorical)

train_levels <- lapply(final_vars$categorical, function(var) levels(train_final[[var]]))
names(train_levels) <- final_vars$categorical

test_final <- prepare_final_data(test_data, final_vars$numeric, final_vars$categorical)

for (var in final_vars$categorical) {
  test_final[[var]] <- factor(test_final[[var]], levels = train_levels[[var]])
}

message("Niveles en Neighborhood (train): ", paste(levels(train_final$Neighborhood), collapse = ", "))
message("Niveles en Neighborhood (test): ", paste(levels(test_final$Neighborhood), collapse = ", "))

final_model <- glm(
  Cara ~ .,
  data = train_final,
  family = binomial,
  control = list(maxit = 100)
)

train_pred <- predict(final_model, train_final, type = "response") > 0.5
test_pred  <- predict(final_model, test_final, type = "response") > 0.5

train_pred_factor <- factor(ifelse(train_pred, "1", "0"), levels = c("0", "1"))
train_true <- factor(as.character(train_final$Cara), levels = c("0", "1"))

test_pred_factor <- factor(ifelse(test_pred, "1", "0"), levels = c("0", "1"))
test_true <- factor(as.character(test_final$Cara), levels = c("0", "1"))

cm_train <- confusionMatrix(table(train_pred_factor, train_true))
cm_test  <- confusionMatrix(table(test_pred_factor, test_true))

print(cm_train)
print(cm_test)


```
El rendimiento del modelo se evalúa mediante validación cruzada y los índices de desempeño resultantes (accuracy de 77,10% en la validación cruzada de 10 folds) son razonables.

El modelo se entrenó con 520 muestras y 72 predictores, presentando un AIC de 364 y una reducción significativa en la devianza residual (de 720.25 a 297.88) lo cual sugiere que la estructura del modelo ajusta la información contenida en los datos. Aunque se indican coeficientes no definidos debido a singularidades (lo que refuerza la existencia de colinealidad en algunos predictores), la evaluación global a través de la validación cruzada (Accuracy y Kappa) proporciona evidencia de que el modelo posee una capacidad moderada a buena para discriminar entre las dos clases ("0" y "1").

Finalmente las matrices de confusión tanto para entrenamiento como para prueba muestran altos índices de exactitud (93,14% y 91,62% respectivamente), alta sensibilidad (alrededor del 94%) y especificidad también elevada (89% y 87,3% respectivamente) para la clasificación de viviendas. Estos resultados, junto con un Kappa por encima de 0.82, confirman que, a nivel predictivo, el modelo es robusto. 
```{r grapghics, echo=FALSE}


set.seed(1234)

train_sizes <- seq(0.1, 1.0, by = 0.1)

train_errors <- numeric(length(train_sizes))
val_errors   <- numeric(length(train_sizes))

for(i in seq_along(train_sizes)) {
  pct <- train_sizes[i]
  n_subset <- floor(pct * nrow(train_final))
  subset_idx <- sample(1:nrow(train_final), size = n_subset)
  train_subset <- train_final[subset_idx, ]

  model_subset <- glm(Cara ~ ., data = train_subset, family = binomial, control = list(maxit = 100))

  pred_train <- predict(model_subset, train_subset, type = "response")
  pred_train_class <- ifelse(pred_train > 0.5, 1, 0)
  # Calcular error: tasa de clasificación errónea en el entrenamiento
  train_errors[i] <- mean(pred_train_class != as.numeric(as.character(train_subset$Cara)))

  pred_val <- predict(model_subset, test_final, type = "response")
  pred_val_class <- ifelse(pred_val > 0.5, 1, 0)
  # Calcular error en validación
  val_errors[i] <- mean(pred_val_class != as.numeric(as.character(test_final$Cara)))
}

learning_curve <- data.frame(
  TrainingSize = train_sizes * 100,  # en porcentaje
  TrainingError = train_errors,
  ValidationError = val_errors
)

learning_curve_melted <- melt(learning_curve, id.vars = "TrainingSize", 
                              variable.name = "Dataset", value.name = "Error")

 ggplot(learning_curve_melted, aes(x = TrainingSize, y = Error, color = Dataset)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "Curvas de Aprendizaje del Modelo de Regresión Logística",
       x = "Porcentaje de Datos de Entrenamiento Utilizados",
       y = "Tasa de Error (Clasificación Incorrecta)") +
  theme_minimal()

```
La gráfica ilustra la evolución de la tasa de error (eje vertical) para el conjunto de entrenamiento (línea roja) y el de validación (línea azul), conforme se incrementa el porcentaje de datos de entrenamiento utilizados (eje horizontal). En los primeros tramos (menores al 30% de datos) el modelo tiende a sobreajustar(training error muy bajo) mientras que el error en validación es más alto debido a que con pocos datos el modelo se adapta excesivamente a la muestra pequeña y no generaliza tan bien. Sin embargo, a medida que aumenta el tamaño de la muestra de entrenamiento ambos errores convergen y se estabilizan, lo cual es una señal de que el modelo no sufre de sobreajuste severo cuando se dispone de suficientes datos. La brecha relativamente estrecha entre los errores de entrenamiento y validación en los últimos tramos (acercándose al 100% de datos) sugiere que el modelo encuentra un buen balance entre complejidad y capacidad de generalización. Este comportamiento respalda la confiabilidad del modelo y confirma que agregar más información de entrenamiento contribuye a una mayor robustez en el desempeño.

7.  Haga un tuneo del modelo para determinar los mejores parámetros, recuerde que los modelos de regresión logística se pueden regularizar como los de regresión lineal.

Ahora que hemos conseguido un mejor modelo gracias a la elección de variables importantes el modelo ha mejorado bastante, pero puede que se puede mejorar un poco más todavía. El modelo previo no incluía el uso de tuning para hiperparámetros, por lo que existe aún rango para una mejora. En este caso estamos usando glmnet para poder relizar el entrenamient ya que en este caso estamos usando glmnet para realizar el entrenamiento ya que nos ayuda a encontrar el mejor balance entre complejidad y rendimiento del modelo, algo que con la regresión logística normal nos costaría más trabajo ajustar manualmente, esto por medio del tuning de las variables alpha lambda. Las condiciones siguen siendo las mismas, mismo set de entrenamiento y prueba junto con el mismo set de variables elegidas.

```{r tunin_model, echo=FALSE}
numeric_vars <- c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
                  "GarageArea", "FullBath", "GarageCars")
categorical_vars <- c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")

# Preparar datos
train_final <- train_data %>%
  select(Cara, all_of(numeric_vars), all_of(categorical_vars)) %>%
  mutate(across(all_of(categorical_vars), as.factor))

x_train <- model.matrix(Cara ~ ., data = train_final)[, -1]  
y_train <- train_final$Cara


test_final2 <- test_data %>%
  select(Cara, all_of(numeric_vars), all_of(categorical_vars)) %>%
  mutate(across(all_of(categorical_vars), ~ factor(., levels = levels(train_final[[cur_column()]]))))

x_test <- model.matrix(Cara ~ ., data = test_final2)[, -1]
y_test <- test_final2$Cara


set.seed(123)
alphas <- seq(0, 1, by = 0.1)  

# Función para encontrar mejor lambda y alpha
tune_glmnet <- function(alpha) {
  cv_model <- cv.glmnet(x_train, y_train, 
                        family = "binomial", 
                        alpha = alpha,
                        type.measure = "class",  # Para accuracy
                        nfolds = 10)
  data.frame(alpha = alpha,
             lambda_min = cv_model$lambda.min,
             lambda_1se = cv_model$lambda.1se,
             accuracy = 1 - min(cv_model$cvm))
}


tune_results <- map_df(alphas, tune_glmnet)


best_alpha <- tune_results$alpha[which.max(tune_results$accuracy)]
best_lambda <- tune_results$lambda_min[which.max(tune_results$accuracy)]

# Modelo final con mejor alpha y lambda
final_glmnet <- glmnet(x_train, y_train, 
                       family = "binomial",
                       alpha = best_alpha,
                       lambda = best_lambda)

# Coeficientes del modelo
# coef(final_glmnet)

# Predicciones
pred_probs <- predict(final_glmnet, newx = x_test, type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

print(paste("Mejor alpha (mezcla Ridge/Lasso):", best_alpha))
print(paste("Mejor lambda (fuerza de regularización):", best_lambda))


```

Ya que en esta situacion no se tiene un único hiperparametro, es importante tener en cuenta la mejor combinación de ambos valores y no únicamente el mejor endividual. Esto se tuvo en cuenta en el entrenamiento y al final se obtuve un valor de alpha de *0.9* y un lambda de *0.007053960638741* lo cual quiere decir que al estar alpha cercano a 1, el modelo priorizó eliminar variables poco importantes, enfocandose en variables más importantes como bien puede ser OverallQual, GrLivArea, entre otros. Ahora en el caso del valor lambda este sugiere que las variables conservadas mantuvieron su poder predictivo y no son limitadas al momento de realizar las predicciones. Por lo que podemos decir que el modelo es bastante selectivo y toma en cuenta las variables más importantes principalmente, manteniendo dichas variables significativas al no restringir la influencia predictiva de estas variables. Con una configuración solida para el entrenamiento con respecto a los hiperparametros, se pudo hacer nuevamente un modelo predictivo para determinar si una casa es cara o no. 

8. Haga un análisis de la eficiencia del algoritmo usando una matriz de confusión. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores, el tiempo y la memoria consumida. Para esto último puede usar “profvis” si trabaja con R

```{r metric_tunin, echo=FALSE}

# Matriz conf
conf_mat <- confusionMatrix(factor(pred_class, levels = c(0, 1)), 
                           factor(y_test, levels = c(0, 1)))
print(conf_mat)

# metricas especificas
accuracy <- conf_mat$overall["Accuracy"]
sensitivity <- conf_mat$byClass["Sensitivity"]  # Recall para clase 0 (no cara)
specificity <- conf_mat$byClass["Specificity"]  # Recall para clase 1 (cara)
precision <- conf_mat$byClass["Pos Pred Value"]  # Precisión para clase 1
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

cat("\n=== MÉTRICAS CLAVE ===",
    "\nAccuracy (Exactitud):", round(accuracy, 4),
    "\nSensibilidad (Clase 0 - 'No Cara'):", round(sensitivity, 4),
    "\nEspecificidad (Clase 1 - 'Cara'):", round(specificity, 4),
    "\nPrecisión (Clase 1):", round(precision, 4),
    "\nF1-Score (Balanceado):", round(f1_score, 4))


conf_df <- as.data.frame(conf_mat$table)
colnames(conf_df) <- c("Predicción", "Real", "Frecuencia")

# Heatmap de errores
ggplot(conf_df, aes(x = Real, y = Predicción, fill = Frecuencia)) +
  geom_tile() +
  geom_text(aes(label = Frecuencia), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Matriz de Confusión: Predicción vs Real",
       subtitle = paste("Accuracy:", round(accuracy, 2))) +
  theme_minimal()

```
Los resultados del modelo tuneado muestran un excelente desempeño en la clasificación de casas para determinar si una vivienda es cara o no. Obteniendo un accuracy del 92.26%, el modelo demuestra una alta capacidad predictiva, acertando en la gran mayoría de los casos. La sensibilidad para la clase 0 ('No Cara') es muy buena, llegando a alcanzar un 95.86% de acierto, lo que indica que el modelo identifica correctamente casi todas las viviendas que no son consideradas caras. Esto es un muy buen logro ya que quiere decir que el modelo es capaz de diferenciar las casas que son menos caras, de las que son más costosas.

Por otro lado, la especificidad del 85.23% para la clase 1 ('Cara') muestra  que el modelo es también bueno al momento de identificar viviendas de alto valor, aunque no es tan bueno como en el caso de las casas no caras, cometiendo algunos errores de clasificación en esta categoría. Además hace sentido que el modelo sea mejor prediciendo las no caras. Recordemos que la regresión logística se basa en probabilidades y habiendo 3 clases distintas, para el modelo obviamente será más sencillo predecir cuando una casa no pertenece a una categoría, aunque esto poría variar dependiendo del umbral de probabilidad que coloquemos para clasificar.Por fines de simplicidad el valor de probabilidad para la clasificación en todos los modelos se dejó en 0.5 (50%) y no fue tomado en cuenta como un hipérparametro para el entrenamiento.

Ahora bien aunque haya disminuido la exactitud para predecir si una casa es cara, la precisión del 92.67% para esta clase muestra que cuando el modelo predice que una vivienda es cara, suele estar en lo correcto. El F1-Score balanceado de 94.24% es también un buen indicador y confirma que el modelo mantiene un buen equilibrio entre precisión y recall en ambas clases.

Estos resultados superan al modelo sin tuning, obteniendo un accuracy ligeramente más alto. La combinación óptima de hiperparámetros (alpha = 0.9 y lambda = 0.007) permitió lograr este balance, donde el modelo es lo suficientemente selectivo para eliminar variables poco relevantes, pero no tan restrictivo como para perder capacidad predictiva. El buen desempeño en las distintas métricas sugiere que el modelo generaliza bien y no está sobreajustado.


```{r performance_test, include=FALSE}
profvis({
  numeric_vars <- c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
                  "GarageArea", "FullBath", "GarageCars")
  categorical_vars <- c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")
  
  # Preparar datos
  train_final <- train_data %>%
    select(Cara, all_of(numeric_vars), all_of(categorical_vars)) %>%
    mutate(across(all_of(categorical_vars), as.factor))
  
  x_train <- model.matrix(Cara ~ ., data = train_final)[, -1]  
  y_train <- train_final$Cara
  
  
  test_final2 <- test_data %>%
    select(Cara, all_of(numeric_vars), all_of(categorical_vars)) %>%
    mutate(across(all_of(categorical_vars), ~ factor(., levels = levels(train_final[[cur_column()]]))))
  
  x_test <- model.matrix(Cara ~ ., data = test_final2)[, -1]
  y_test <- test_final2$Cara
  
  
  set.seed(123)
  alphas <- seq(0, 1, by = 0.1)  
  
  # Función para encontrar mejor lambda y alpha
  tune_glmnet <- function(alpha) {
    cv_model <- cv.glmnet(x_train, y_train, 
                          family = "binomial", 
                          alpha = alpha,
                          type.measure = "class",  # Para accuracy
                          nfolds = 10)
    data.frame(alpha = alpha,
               lambda_min = cv_model$lambda.min,
               lambda_1se = cv_model$lambda.1se,
               accuracy = 1 - min(cv_model$cvm))
  }
  
  
  tune_results <- map_df(alphas, tune_glmnet)
  
  
  best_alpha <- tune_results$alpha[which.max(tune_results$accuracy)]
  best_lambda <- tune_results$lambda_min[which.max(tune_results$accuracy)]
  
  # Modelo final con mejor alpha y lambda
  final_glmnet <- glmnet(x_train, y_train, 
                         family = "binomial",
                         alpha = best_alpha,
                         lambda = best_lambda)
  
  # Coeficientes del modelo
  # coef(final_glmnet)
  
  # Predicciones
  pred_probs <- predict(final_glmnet, newx = x_test, type = "response")
  pred_class <- ifelse(pred_probs > 0.5, 1, 0)

})
```

Además de analizar la matríz de confusión y las distintas métricas de desempeño, también probamos a utilizar la herramienta de profviz para poder conocer que tan bueno es el algoritmo utilizado a nivel de uso de recursos, tiempo y eficiencia. En esta prueba inicialmente no tomamos en cuenta la parte del tuning dentro del análisis de ejecución, ya que el tuning no forma parte de la función de entrenamiento de glmnet() como tal. 

Pero el problema con esto fue que los resultados de este análisis al final no eran tan significativos como se esperaba. En la mayoría de los casos la ejecución del algoritmo era rápida, generalmente menos de un segundo 40 ms para ser exactos y 0.6MB de memoria. Suponemos es por el hecho de no incluir el tiempo del tuning el algoritmo manejaba una gran eficiencia media vez se tengan ya los hiperparámetros o no se haga tunning alguno. Ahora bien, una vez integrado el algoritmo y código completo (incluyendo el tunning) la diferencia fue mucho más notoria. Logramos ver que en total todo el algoritmo duraba 5.8 segundos aproximadamente. El proceso de ajuste de hiperparámetros consumió 299 MB y demoró casi un segundo, mientras que la parte de entrenamiento con glmnet() consumió 4MB y tardó 10ms. El entrenamiento es rápido, pero el tunning si crece un poco en tiempo, pero la verdad es que no es una cantidad de tiempo extensa. Con los resultado obtenidos hasta el momento es posible entonces profundizar un poco más y comparar los modelos.

9. Determine cual de todos los modelos es mejor, puede usar AIC y BIC para esto, además de los parámetros de la matriz de confusión y los del profiler

En este caso tuvimos distintos resultados/modelos para la predicción si una casa es cara o no, como vimos en todos el análisis previos. Realizamos un modelo sin mayores ajustes, uno con validación cruzada, selección de variables y otro con tunning, pero en este análisis preferimos enfocarnos en los úlitmos 2 modelos realizados. La razón es que realmente estos 2 son los mejores y los primeros 2 modelos son significativamente peores, obteniendo un accuracy de 0.7345955 para el primer modelo y un accuracy de 0.8633 en el caso del segundo, mientras que el modelo 3 y 4 tuvieron métricas por encima de 90 para clasificar si una casa era cara o no. Es por eso que nos enfocamos en estas, comparando en una tabla el Akaike Information Criterion (AIC) y Bayesian Information Criterion (BIC). 

```{r comparison_part2, echo=FALSE}
# Función para extraer métricas de una matriz de conf
get_metrics_from_cm <- function(conf_matrix) {
  data.frame(
    Accuracy = conf_matrix$overall["Accuracy"],
    Sensitivity = conf_matrix$byClass["Sensitivity"],
    Specificity = conf_matrix$byClass["Specificity"],
    F1 = conf_matrix$byClass["F1"]
  )
}

# Extraer métricas de las matrices que ya tenemos de antes
metrics_modelo_2 <- get_metrics_from_cm(cm_test)   # Modelo con selección de variables (GLM)
metrics_modelo_3 <- get_metrics_from_cm(conf_mat)  # Modelo con tuning (GLMNet)

# Crear tabla comparativa
comparison <- data.frame(
  Modelo = c("Modelo 3 (GLM)", "Modelo 4 (GLMNet)"),
  AIC = c(AIC(final_model), NA),  
  BIC = c(BIC(final_model), NA),  
  Accuracy = c(metrics_modelo_2$Accuracy, metrics_modelo_3$Accuracy),
  Sensitivity = c(metrics_modelo_2$Sensitivity, metrics_modelo_3$Sensitivity),
  Specificity = c(metrics_modelo_2$Specificity, metrics_modelo_3$Specificity),
  F1 = c(metrics_modelo_2$F1, metrics_modelo_3$F1)
)

# Añadir AIC/BIC aproximado para glmnet en caso sea necsario
if (!is.null(final_glmnet)) {
  log_lik <- sum(dbinom(y_test, 
                       size = 1, 
                       prob = predict(final_glmnet, newx = x_test, type = "response"), 
                       log = TRUE))
  k <- sum(coef(final_glmnet) != 0)  
  n <- length(y_test)
  
  comparison$AIC[2] <- -2 * log_lik + 2 * k
  comparison$BIC[2] <- -2 * log_lik + k * log(n)
}

print(comparison)

```

Ahora teniendo esta información es posible diferenciar mejor cual de los ejercicios ha sido el mejor. El Modelo 4, en el que se usó GLMNet, muestra ciertas ventajas. Con un AIC de 220.59 y BIC de 326.78 notablemente más bajos que los del Modelo 3 con 472.18 y 595.40 respectivamente. Esto indica un mejor equilibrio entre complejidad y ajuste a los datos. Esta mejora se refleja también en el accuracy, ya que al final se tuvo un 92.26% vs 91.62%, donde el modelo con tunning supera ligeramente al estándar. Sin embargo, lo más destacable es el F1-Score donde el modelo 4 gana al comparar los valores de 94.24% vs 93.24%, que confirma una mayor capacidad para mantener el balance entre precisión y recall en sus predicciones.

Las métricas específicas por clase muestran un comportamiento interesante, ya que el Modelo 4 logra una sensibilidad bastante alta con 95.86%, para identificar correctamente viviendas no caras (clase 0), aunque con una pequeña disminución en especificidad. El modelo 3 gana al comparar los valores de 85.23% vs 87.31% para detectar viviendas caras (clase 1). Este trade-off parece valioso, ya que el GLMNet compensa esta diferencia con una mayor precisión general y un mejor desempeño global. Entonces al final el modelo 4 resulta ser menos hábil en la clasificación de viviendas que si son caras a cambio de tener un mayor accuracy y generalizar mejor en sus predicciones, lo cual es intercambio bastante válido.

Ahora bien, estos modelos que hemos hecho hasta el momento han tenido métricas muy altas en comparación a otros modelos y algoritmos probados en el pasado. Con estos modelos se ha hecho cross validation, tunning y se ha verificado que no hay overfitting, lo cuál es bueno. Pero hay que tener una considreación qu eestamos haciendo únicamente una clasificación binaria donde decimos que una casa pertenece o no a un tipo de precio (cara en este caso), mientras que en los demás algoritmos se calsificaban las casas en 3 categorías distintas. Es por eso que ahora lo que corresponde es probar utilizar regresión logística para clasificar las casas en barata, media y cara.

10. Haga un modelo de regresión logística para la variable categórica para el precio de las casas (categorías: barata, media y cara). Asegúrese de tunearlo para obtener el mejor modelo posible.

Por la naturaleza del algoritmo, se sabe que la regresión logística no es un clasificador directo. La regresión logística provee una probabilidad de entre 0 y 1 de que un registro pertenezca o no a una clase. Hemos solucionado esto hasta el momento colocando un umbral, donde si la probabilidad es mayoyr al 50% pertenece a una clase y en General esto seguirá siendo así, con la diferencia de que en la configuración del entrenamiento, se debe cambiar el "family" a multinomial. Las condiciones son las mismas, mismo set de prueba y entrenamiento, mismas variables y con tunning para poder obtener el mejor modelo posible.


```{r clasification_logisti_regresion, echo=FALSE}
head(train_data)
clasificaciones <- table(train_data$CategoriaPrecio)
print(clasificaciones)

train_data$CategoriaPrecio <- factor(train_data$CategoriaPrecio, 
                                    levels = c("barata", "estandar", "cara"))
test_data$CategoriaPrecio <- factor(test_data$CategoriaPrecio, 
                                   levels = levels(train_data$CategoriaPrecio))

# Seleccionar variables (usaremos las mismas del ejercicio anterior)
numeric_vars <- c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
                  "GarageArea", "FullBath", "GarageCars")
categorical_vars <- c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")

# Crear fórmula
formula <- as.formula(paste("CategoriaPrecio ~", 
                           paste(c(numeric_vars, categorical_vars), collapse = " + ")))

# Preparar matrices para glmnet
x_train <- model.matrix(formula, data = train_data)[, -1]  # Eliminar intercept
y_train <- train_data$CategoriaPrecio

test_complete <- test_data[complete.cases(test_data[, all.vars(formula)]), ]
y_test_complete <- test_complete$CategoriaPrecio

x_test <- model.matrix(formula, data = test_complete)[, -1]


# Tuneo de alpha (mezcla) y lambda (penalización) con validación cruzada
set.seed(123)
cv_multinom <- cv.glmnet(x_train, y_train, 
                        family = "multinomial",  # Para >2 clases
                        alpha = 0.5,             # Elastic Net (0.5 = balance)
                        type.measure = "class",  # Métrica: error de clasificación
                        nfolds = 10)

# Mejores parámetros
best_lambda <- cv_multinom$lambda.min
cat("Mejor lambda:", best_lambda, "\n")

# Modelo final con mejores parámetros
final_multinom <- glmnet(x_train, y_train, 
                        family = "multinomial",
                        alpha = 0.5,
                        lambda = best_lambda)

# Coeficientes (por categoría)
# print(coef(final_multinom))

# Predicciones en test
pred_class_multi <- predict(final_multinom, newx = x_test, type = "class")

cat("Longitud de pred_class_multi:", length(pred_class_multi), "\n")
cat("Longitud de y_test:", length(y_test_complete), "\n")

# Asegurar que los niveles de referencia y predicción coincidan
common_levels <- intersect(levels(y_test_complete), unique(pred_class_multi))

# Filtrar solo las clases presentes
y_test_filtered <- factor(y_test_complete, levels = common_levels)
pred_class_filtered <- factor(pred_class_multi, levels = common_levels)

# Verificar niveles
cat("Niveles en y_test:", levels(y_test_filtered), "\n")
cat("Niveles en predicciones:", levels(pred_class_filtered), "\n")

# Matriz de confusión corregida
conf_mat_tuned <- confusionMatrix(pred_class_filtered, y_test_filtered)
print(conf_mat_tuned)
# Métricas por clase
cat("Accuracy:", conf_mat_tuned$overall["Accuracy"], "\n")
print(conf_mat_tuned$byClass[, c("Sensitivity", "Specificity", "F1")])

```

El modelo de clasificación multiclase logró un desempeño sólido, con un accuracy general del 83.49%, superando significativamente a un modelo que predice de manera aleatoria. Los resultados muestran que el modelo tiene particular habilidad para identificar correctamente las casas de las categorías "barata" con sensibilidad de 88.65% y las casas clasificadas como "cara" con sensibilidad de 89.26%. Pero por otra parte parece que hay una mayor dificultad con la clase intermedia "estándar" ya que la sensibilidad es considerablemente más baja llegando a un 72.60%. Esto se refleja en la matriz de confusión, donde se observan más errores entre las categorías "estándar" y las relacionadas con este. 

Este comportamiento hasta cierto punto es esperado, tomando en cuenta la cantidad de datos que tenemos y que este problema se dió justamente en casi todos los modelos independientemente del algoritmo. Suponemos puede haber varias casas que estén muy cerca de ser caras o baratas y dichos datos son clasificados erreneamente. Ya sea por este motivo o bien por datos atípicos, como una casa a un precio más bajo o alto de lo normal por ejemplo, la clase estándar sigue siendo la más complicada de clasificar.

Ahora con respecto al valor kappa de 0.7523 este indica que el modelo supera por bastante las expectativas de un clasificador aleatorio. Las métricas por clase muestran un buen equilibrio entre sensibilidad y especificidad en todas las categorías, siendo especialmente destacable el desempeño para la clase "cara" que combina una alta sensibilidad (89.26%) con una excelente especificidad (94.77%). El lambda óptimo encontrado (0.0038) sugiere que el modelo se benefició de una regularización moderada, permitiendo un buen balance entre complejidad y capacidad predictiva. El valor de alpha no formó parte del tuning en este caso y por ende se notó más rápido al momento de ejecutar aunque la diferencia no fue tan significativa.

11. Compare la eficiencia del modelo anterior con los de clasificación de las entregas anteriores ¿Cuál se demoró más en procesar?¿Cuál se equivocó más?¿Cuál se equivocó menos?¿por qué?

Esto se respinderá mejor en el documento final, ya que colocar los resultados de los módelos junto con el código sería más tardado, largo y redundante. Pero el análisis completo se encontrará en el documento final de entrega.