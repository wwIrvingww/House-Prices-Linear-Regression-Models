---
title: "Logistic_regression"
author: "Irving, Chuy"
date: "2025-04-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(car)
library(reshape2)
library(ggplot2)
library(forcats)
library(glmnet)
library(caret)
library(profvis)
library(nnet) 
```

2.Use los mismos conjuntos de entrenamiento y prueba que utilizó en las hojas anteriores.
```{r load_data, echo=FALSE}
#train data ya tiene el 70 y test_data el 30%
train_data <- read.csv("../train_final.csv")
test_data <- read.csv("../test_final.csv")
```
Se  cargó la data que se ha usado en entregas anteriores.

```{r remove_columns, echo=FALSE}
train_data <- train_data %>% select(-Condition2, -RoofMatl, -Exterior2nd, -Electrical)
test_data <- test_data %>% select(-Condition2, -RoofMatl, -Exterior2nd, -Electrical)
```
Se removieron las columnas problematicas

```{r remove_na_columns, echo=FALSE}
pct_na <- sapply(train_data, function(x) mean(is.na(x))*100)
drop_cols <- names(pct_na[pct_na > 75])
train_data <- train_data %>% select(-all_of(drop_cols))
test_data  <- test_data  %>% select(-all_of(drop_cols))
```
Se remueven las columnas que no dan información útil.

```{r discretization, echo=FALSE}
breaks <- quantile(train_data$SalePrice, probs = c(0,1/3,2/3,1), na.rm=TRUE)
labels <- c("barata","estandar","cara")

train_data$CategoriaPrecio <- cut(train_data$SalePrice, breaks=breaks, labels=labels, include.lowest=TRUE)
test_data$CategoriaPrecio  <- cut(test_data$SalePrice,  breaks=breaks, labels=labels, include.lowest=TRUE)

```
Creamos las tres categorías de las casas (barata, estándar, cara.)

1. Cree una variable dicotómica por cada una de las categorías de la variable respuesta categórica que creó en hojas anteriores. Debería tener 3 variables dicotómicas (valores 0 y 1) una que diga si la vivienda es cara o no, media o no, económica o no.
```{r dicotomic_variable, echo=TRUE}
# Para el conjunto de entrenamiento
train_data <- train_data %>%
  mutate(
    Cara = ifelse(CategoriaPrecio == "cara", 1, 0),
    Estandar = ifelse(CategoriaPrecio == "estandar", 1, 0),
    Economica = ifelse(CategoriaPrecio == "barata", 1, 0)
  )

# Para el conjunto de prueba
test_data <- test_data %>%
  mutate(
    Cara = ifelse(CategoriaPrecio == "cara", 1, 0),
    Estandar = ifelse(CategoriaPrecio == "estandar", 1, 0),
    Economica = ifelse(CategoriaPrecio == "barata", 1, 0)
  )
```
Se crearon las tres variables dicotómicas para las categorías `barata`, `estandar` y `cara`

3. Elabore un modelo de regresión logística para conocer si una vivienda es cara o no, utilizando el conjunto
de entrenamiento y explique los resultados a los que llega. El experimento debe ser reproducible por lo
que debe fijar que los conjuntos de entrenamiento y prueba sean los mismos siempre que se ejecute el
código. Use validación cruzada.

```{r process_data, echo=FALSE}
numeric_cols <- train_data %>% select(where(is.numeric)) %>% names()

all_cat <- train_data %>% select(where(~!is.numeric(.))) %>% names()
valid_cat <- all_cat[
  sapply(all_cat, function(col) {
    n_distinct(train_data[[col]]) > 1 && n_distinct(test_data[[col]]) > 1
  })
]

preNum <- preProcess(train_data[numeric_cols], method = "medianImpute")
train_data[numeric_cols] <- predict(preNum, train_data[numeric_cols])
test_data[numeric_cols]  <- predict(preNum, test_data[numeric_cols])

encoder <- dummyVars(~ ., data = train_data[valid_cat], fullRank = TRUE)
train_cat <- predict(encoder, train_data[valid_cat]) %>% as.data.frame()
test_cat  <- predict(encoder, test_data[valid_cat]) %>% as.data.frame()


missing <- setdiff(names(train_cat), names(test_cat))
if(length(missing) > 0){
  test_cat[missing] <- 0
  test_cat <- test_cat[names(train_cat)]
}

train_encoded <- bind_cols(train_data[numeric_cols], train_cat, 
                           CategoriaPrecio = train_data$CategoriaPrecio)
test_encoded  <- bind_cols(test_data[numeric_cols],  test_cat,  
                           CategoriaPrecio = test_data$CategoriaPrecio)

```
Antes de elaborar el modelo, se realizó un procesamiento de la data para que sea compatible con el modelo que vamos a utilizar. Ya que el modelo no puede contener NAN's y el número de observaciones que contenía NAN's era menor a 100, se optó por omitir esas filas.



```{r logistic_regression, echo=FALSE}
set.seed(1234)

model_data <- train_data %>% select(-SalePrice, -CategoriaPrecio, -Estandar, -Economica)

model_data <- na.omit(model_data)

train_control <- trainControl(method = "cv", number = 10)

logistic_model <- train(as.factor(Cara) ~ ., 
                        data = model_data, 
                        method = "glm", 
                        family = "binomial", 
                        trControl = train_control)
print(logistic_model)


```
El primer aspecto a destacar es el aviso proporcionado por la librería: "glm.fit: algorithm did not converge". Esto indica que el algoritmo iterativo utilizado para ajustar la regresión logística no logró alcanzar una convergencia completa. En otras palabras, después de varias iteraciones, el método no pudo encontrar un conjunto de parámetros estables. Esta situación puede ocurrir cuando hay variables altamente correlacionadas (multicolinealidad) o cuando se presenta una separación completa o casi completa de las clases en algunos predictores.

Otro aviso relevante es "prediction from rank-deficient fit; attr(*, 'non-estim') has doubtful cases". Este mensaje sugiere que el modelo es deficiente en rango lo que significa que existen redundancias entre las variables predictoras, impidiendo que algunos coeficientes se estimen de manera única. Esto suele suceder cuando hay variables con alta correlación o cuando se incluyen más predictores de los que la información en la muestra puede soportar.

En cuanto a las estadísticas obtenidas, se alcanzó una `accuracy` de 0.7345955 y un `kappa` de 0.4682196, lo que sugiere que, a pesar de los avisos, el modelo presenta un desempeño moderado al clasificar las viviendas como "caras" o "no caras". Aunque el rendimiento predictivo es aceptable, los avisos indican la necesidad de ser cauteloso al interpretar los coeficientes y confiar en la estabilidad del modelo. Por lo tanto, se procederá a realizar un análisis más detallado para investigar la posible existencia de multicolinealidad.

4. Analice el modelo. Determine si hay multicolinealidad en las variables, y cuáles son las que aportan al
modelo, por su valor de significación. Haga un análisis de correlación de las variables del modelo y
especifique si el modelo se adapta bien a los datos.

```{r modelo_summary, echo=TRUE}
full_model <- glm(as.factor(Cara) ~ ., data = model_data, family = "binomial")
summary(full_model)
```

```{r predictors_non_alias, echo=TRUE, warning=FALSE, message=FALSE}
coef_names <- names(coef(full_model))

aliased <- alias(full_model)$Complete
non_alias <- coef_names[!coef_names %in% rownames(aliased)]

predictors_non_alias <- non_alias[non_alias != "(Intercept)"]

print("Predictors no aliasados:")
print(predictors_non_alias)
```

```{r settings_model, echo=TRUE}
available_predictors <- predictors_non_alias[predictors_non_alias %in% names(model_data)]
print("Predictores disponibles en model_data:")
print(available_predictors)

available_predictors_backticks <- paste0("`", available_predictors, "`")

fmla_non_alias <- as.formula(paste("as.factor(Cara) ~", 
                                   paste(available_predictors_backticks, collapse = " + ")))
print("Fórmula del modelo sin predictores aliasados y con variables disponibles:")
print(fmla_non_alias)

full_model_non_alias <- glm(fmla_non_alias, data = model_data, family = "binomial")

summary(full_model_non_alias)
```

```{r vif_stimate, echo=TRUE}

aliased <- alias(full_model)$Complete
print("Variables colineales (alias):")
print(aliased)

coef_names <- names(coef(full_model))

non_alias <- coef_names[!coef_names %in% rownames(aliased)]
print("Coeficientes no aliasados:")
print(non_alias)

available_predictors <- predictors_non_alias[predictors_non_alias %in% names(model_data)]
print("Predictors no alias presentes en model_data:")
print(available_predictors)

available_predictors_backticks <- paste0("`", available_predictors, "`")
fmla_non_alias <- as.formula(paste("as.factor(Cara) ~", 
                                   paste(available_predictors_backticks, collapse = " + ")))
print("Fórmula del modelo sin predictores aliasados y solo con variables disponibles:")
print(fmla_non_alias)

full_model_non_alias <- glm(fmla_non_alias, data = model_data, family = "binomial")

vif_values <- vif(full_model_non_alias)
print("Valores VIF para el modelo sin coeficientes aliasados:")
print(vif_values)

```

Dentro de la función VIF se calcularon distintos valores de los cuales se pueden discutir los siguientes aspectos: variables como `Id` con un VIF de 1.16, `MSSubClass` con 1.59, `LotFrontage` con 1.66 y `LotArea` con 1.41 presentan coeficientes entre 1 y 3, lo que indica poca o ninguna colinealidad significativa en esos casos. Sin embargo, se observan valores moderadamente altos, como `X1stFlrSF` con 7.39 y `X2ndFlrSF` con 8.15, que sugieren una correlación con otras variables. 

Por otro lado, hay variables que presentan valores muy altos superiores a 10 como `YearBuilt` con aproximadamente 10.65, `GarageYrBlt` con 10.06, `BsmtFinSF1` con 9.58 y `BsmtUnfSF` con 8.62. Estos valores indican un problema de multicolinealidad fuerte en estos predictores. No obstante esta alta multicolinealidad puede ser razonable en ciertos casos, como la relación entre el año de construcción y el año de construcción del garaje, así como la alta correlación entre las medidas de superficie del sótano y la superficie del primer piso.

En general aunque muchas de las variables tienen VIF razonablemente bajos, la presencia de algunos predictores con VIF superiores a 10 (o cercanos a esos valores) es una señal de redundancia en la información. Esto podría hacer que las estimaciones de los coeficientes sean inestables y dificultar la interpretación individual de estos.


```{r correlacion_predictors_fixed, echo=TRUE, warning=FALSE, message=FALSE}
predictors_numeric <- model_data %>% 
  select(-Cara) %>% 
  select(where(is.numeric))

corr_matrix <- cor(predictors_numeric, use = "complete.obs")
print("Matriz de correlación (redondeada):")
print(round(corr_matrix, 2))
```

Al analizar la matriz de correlación se observa que algunas variables categóricas como el tipo de construcción (`MSSubClass`) presentan correlaciones moderadas con otras medidas. Por ejemplo, hay una correlación negativa moderada de aproximadamente –0.32 entre `MSSubClass` y `LotFrontage`, sugiriendo diferencias en el frente de lote según la categoría estructural de las viviendas.

Las variables relacionadas con la calidad y condición de la vivienda también destacan. En particular “OverallQual” muestra correlaciones positivas significativas con “YearBuilt” (0.53) y “YearRemodAdd” (0.57), indicando que las viviendas más modernas o recientemente remodeladas tienden a tener calidades percibidas más altas.

En cuanto a las medidas de área, “TotalBsmtSF” y “X1stFlrSF” tienen una correlación muy alta (cercana a 0.90), sugiriendo que ambas miden aspectos del tamaño de la vivienda y presentan redundancia contribuyendo a la multicolinealidad del modelo. Otros indicadores de área, como “LotFrontage”, “LotArea” y “GrLivArea”, también muestran correlaciones moderadas entre sí.

Las variables temporales, “YearBuilt” y “YearRemodAdd”, tienen una correlación alta (0.69), indicando que en muchas viviendas el año de construcción y el de remodelación están estrechamente relacionados.

Aunque muchas variables presentan correlaciones moderadas o bajas existen grupos de variable especialmente relacionadas con el tamaño y la estructura de la vivienda que están fuertemente correlacionadas. Esto refuerza la preocupación por la multicolinealidad, como se evidenció en los altos valores de VIF. La matriz de correlación sugiere que una reducción de dimensiones o una selección cuidadosa de predictores podría mejorar la interpretabilidad del modelo sin comprometer su capacidad predictiva.

```{r heatmap_correlacion, echo=FALSE, warning=FALSE, message=FALSE}
melted_corr <- melt(corr_matrix)

ggplot(data = melted_corr, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", 
                       midpoint = 0, limit = c(-1, 1), name = "Correlación") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        axis.title = element_blank()) +
  labs(title = "Heatmap: Matriz de correlación de los predictores")
```
El heatmap de la matriz de correlación confirma de manera visual los hallazgos numéricos previos. Las áreas más intensamente coloreadas en rojo indican pares de variables con correlaciones positivas altas, mientras que los bloques en azul reflejan correlaciones negativas notables. En particular puede apreciarse un clúster de variables que representan la dimensión de la vivienda (por ejemplo, TotalBsmtSF, X1stFlrSF, GarageCars, GarageArea) mostrando altas correlaciones entre sí, lo que respalda la existencia de multicolinealidad detectada por los valores de VIF. De forma similar, otras zonas del mapa exhiben correlaciones elevadas entre las variables temporales (YearBuilt, YearRemodAdd, GarageYrBlt) y los indicadores de calidad (OverallQual, OverallCond). 

5. Utilice el modelo con el conjunto de prueba y determine la eficiencia del algoritmo para clasificar.  
```{r model_test, echo=FALSE}
# Predecir las probabilidades para el conjunto de prueba usando el modelo ajustado (full_model_non_alias)
pred_probs <- predict(full_model_non_alias, newdata = test_data, type = "response")

# Convertir las probabilidades en clases (usamos un umbral de 0.5)
pred_class <- ifelse(pred_probs >= 0.5, 1, 0)

# Asegurarse que tanto las predicciones como la variable real de respuesta sean factores
pred_class <- as.factor(pred_class)
actual_class <- as.factor(test_data$Cara)

conf_mat <- confusionMatrix(pred_class, actual_class)
print(conf_mat)

```
La matriz de confusión y las estadísticas resultantes indican que el modelo de clasificación binaria (para predecir si una vivienda es "cara" o no) tiene un desempeño robusto. Con una **exactitud del 86.33%** y un intervalo de confianza del 95% entre 82.76% y 89.41%, el modelo supera significativamente la tasa de no información (66.06%), lo que se refleja en un p-valor muy bajo (< 2.2e-16) al comparar con la tasa de no información. Además, el valor de **Kappa (0.6806)** sugiere un acuerdo sustancial entre las predicciones y las observaciones reales, más allá de lo que se esperaría por azar.

En detalle, la **sensibilidad del 94.48%** (dado que la clase positiva se define como la etiqueta 0) indica que el modelo identifica correctamente la gran mayoría de las viviendas que pertenecen a la clase positiva. La **especificidad del 70.47%** evidencia que, aunque la identificación de la clase contraria (viviendas "cara", en este contexto) es moderada, el desempeño en general es bastante equilibrado, como lo refleja la **exactitud balanceada del 82.48%**. Los valores altos de las métricas de valor predictivo (Pos Pred Value: 86.16% y Neg Pred Value: 86.78%) refuerzan la idea de que el modelo es confiable al asignar correctamente la clase a nuevas observaciones.

Finalmente, el resultado del Test de McNemar (p-value = 0.0004909) indica que las discrepancias en los errores de clasificación son estadísticamente significativas, lo que añade evidencia a la robustez del modelo. En conjunto, estos resultados respaldan que el modelo tiene una capacidad predictiva sólida para discriminar entre viviendas clasificadas como "cara" y "no cara".


6. Explique si hay sobreajuste (overfitting) o no (recuerde usar para esto los errores del conjunto de prueba
y de entrenamiento). Muestre las curvas de aprendizaje usando los errores de los conjuntos de
entrenamiento y prueba
```{r prepare_data, echo=FALSE}
prep_data <- function(data) {
  # Eliminar columnas no relevantes y con muchos NA
  data <- data %>%
    select(-any_of(c("SalePrice", "CategoriaPrecio", "Estandar", "Economica"))) %>%
    select(-where(~mean(is.na(.)) > 0.3)) %>%
    mutate(across(where(is.character), as.factor))
  
  # Eliminar filas con NA en la respuesta
  data <- data %>% filter(!is.na(Cara))
  
  return(data)
}

train_prep <- prep_data(train_data)
test_prep <- prep_data(test_data)

select_numeric_vars <- function(train_data, test_data, n_vars = 7) {
  numeric_vars <- train_data %>%
    select(where(is.numeric), -Cara) %>%
    names()

  cor_values <- sapply(numeric_vars, function(x) {
    cor(train_data[[x]], as.numeric(train_data$Cara), use = "complete.obs")
  })

  selected_num <- names(sort(abs(cor_values), decreasing = TRUE)[1:n_vars])

  cor_test <- sapply(selected_num, function(x) {
    cor(test_data[[x]], as.numeric(test_data$Cara), use = "complete.obs")
  })

  keep_num <- selected_num[sign(cor_values[selected_num]) == sign(cor_test)]
  
  return(keep_num)
}

numeric_selected <- select_numeric_vars(train_prep, test_prep, 7)
message("Variables numéricas seleccionadas: ", paste(numeric_selected, collapse = ", "))

select_categorical_vars <- function(train_data, test_data, n_vars = 7) {
  cat_vars <- train_data %>%
    select(where(is.factor), -Cara) %>%
    names()

  chi2_results <- sapply(cat_vars, function(x) {
    tbl <- table(train_data[[x]], train_data$Cara)
    if (any(dim(tbl) < 2)) return(0)
    test <- chisq.test(tbl, simulate.p.value = TRUE)
    -log10(test$p.value)
  })

  selected_cat <- names(sort(chi2_results, decreasing = TRUE)[1:n_vars])
  
  chi2_test <- sapply(selected_cat, function(x) {
    tbl <- table(test_data[[x]], test_data$Cara)
    if (any(dim(tbl) < 2)) return(0)
    test <- chisq.test(tbl, simulate.p.value = TRUE)
    -log10(test$p.value)
  })

  keep_cat <- selected_cat[chi2_test > 2] # p < 0.01
  
  return(keep_cat)
}

categorical_selected <- select_categorical_vars(train_prep, test_prep, 7)
message("Variables categóricas seleccionadas: ", paste(categorical_selected, collapse = ", "))


validate_with_rf <- function(train_data, num_vars, cat_vars) {
  if (!require("randomForest")) install.packages("randomForest")
  library(randomForest)
  
  model_data <- train_data %>%
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(across(all_of(cat_vars), as.factor),
           Cara = as.factor(Cara))  
  

  set.seed(123)
  rf_model <- randomForest(Cara ~ ., data = model_data, importance = TRUE)
  
  imp_df <- as.data.frame(importance(rf_model)) %>%
    mutate(Variable = rownames(.)) %>%
    arrange(desc(MeanDecreaseAccuracy))
  
  return(list(model = rf_model, importance = imp_df))
}

rf_result <- validate_with_rf(train_prep, numeric_selected, categorical_selected)


print(rf_result$importance)

create_final_datasets <- function(train_data, test_data, num_vars, cat_vars) {
  
  final_train <- train_data %>%
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(across(all_of(cat_vars), as.factor))
  
  final_test <- test_data %>%
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(across(all_of(cat_vars), as.factor))
  
  for (var in cat_vars) {
    if (nlevels(final_train[[var]]) < 2) {
      final_train <- final_train %>% select(-all_of(var))
      final_test <- final_test %>% select(-all_of(var))
      message("Variable categórica ", var, " eliminada por tener un solo nivel")
    }
  }
  
  return(list(
    train = final_train,
    test = final_test,
    numeric_vars = num_vars,
    categorical_vars = intersect(cat_vars, names(final_train))
  ))
}


select_final_vars <- function(importance_df, num_vars, cat_vars, n_final = 14) {
  final_vars <- importance_df$Variable[1:min(n_final, nrow(importance_df))]
  
  final_num <- intersect(final_vars, num_vars)
  final_cat <- intersect(final_vars, cat_vars)
  
  remaining <- n_final - length(final_num) - length(final_cat)
  if (remaining > 0) {

    all_vars <- c(num_vars, cat_vars)
    remaining_vars <- setdiff(all_vars, c(final_num, final_cat))
    to_add <- importance_df$Variable %>%
      intersect(remaining_vars) %>%
      head(remaining)
    
    final_num <- c(final_num, intersect(to_add, num_vars))
    final_cat <- c(final_cat, intersect(to_add, cat_vars))
  }
  
  return(list(num = final_num, cat = final_cat))
}

final_vars <- select_final_vars(rf_result$importance, numeric_selected, categorical_selected)


final_data <- create_final_datasets(
  train_prep, test_prep, 
  final_vars$num, final_vars$cat
)


varImpPlot(rf_result$model, main = "Importancia de Variables Seleccionadas")

message("\n=== VARIABLES FINALES SELECCIONADAS ===")
message("Numéricas (", length(final_data$numeric_vars), "):")
message(paste(final_data$numeric_vars, collapse = ", "))

message("\nCategóricas (", length(final_data$categorical_vars), "):")
message(paste(final_data$categorical_vars, collapse = ", "))

message("\nTop 5 variables más importantes:")
print(head(rf_result$importance, 5))


selected_vars <- list(
  numeric = final_data$numeric_vars,
  categorical = final_data$categorical_vars,
  importance = rf_result$importance
)


message("\nVerificación de niveles categóricos:")
for (var in final_data$categorical_vars) {
  message(var, ": ", nlevels(final_data$train[[var]]), " niveles")
}

message("\nDistribución de clases:")
table(final_data$train$Cara)
```
Se realiza la preparación y limpieza de datos eliminando columnas con demasiados valores perdidos (>30%), descartando variables no relevantes (`SalePrice`, `CategoriaPrecio`, etc.) y transformando a factores aquellas columnas de tipo carácter. Seguidamente, se implementan dos funciones de selección de variables: (1) `select_numeric_vars` para escoger las variables numéricas más correlacionadas con la variable respuesta `Cara` en ambos conjuntos (entrenamiento y prueba), y (2) `select_categorical_vars` para filtrar las variables categóricas con mayor relevancia estadística (p-valor bajo en test de Chi-cuadrado). Estas selecciones permiten reducir dimensionalidad y enfocarse en los predictores potencialmente más influyentes.

Tras ello se entrena un modelo de Random Forest (`validate_with_rf`) con las variables filtradas, obteniendo una tabla de importancia de variables (`importance`) que determina cuáles predictores aportan más a la clasificación. En particular, se ve que variables como `GrLivArea`, `X1stFlrSF`, `Neighborhood` y `OverallQual` destacan por su contribución, lo que coincide con la intuición de que tanto la superficie habitable como la calidad y la ubicación de la vivienda son determinantes relevantes para el costo o el tipo de la casa.

Finalmente con la función `create_final_datasets` se construyen los data frames finales para entrenamiento y prueba, asegurando que las columnas categóricas tengan niveles adecuados y que no existan variables categóricas con un solo nivel (que no aportarían información). El reporte de variables finales (7 numéricas y 5 categóricas) y los niveles de cada categoría** confirma que los datos están listos para el modelado, mientras que la **distribución de clases (687 para la clase 0 y 334 para la clase 1) sugiere un ligero desbalance pero no excesivo. 
```{r, echo=FALSE}

prepare_final_data <- function(data, num_vars, cat_vars) {

  data <- data %>% 
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(
      across(all_of(cat_vars), ~ forcats::fct_lump_prop(as.factor(.), prop = 0.05, other_level = "OTROS")),
      Cara = as.factor(Cara)
    )
  return(data)
}

train_final <- prepare_final_data(train_data, final_vars$numeric, final_vars$categorical)
test_final  <- prepare_final_data(test_data,  final_vars$numeric, final_vars$categorical)

for(var in final_vars$categorical){
  test_final[[var]] <- factor(test_final[[var]], levels = levels(train_final[[var]]))
}

message("Niveles en Neighborhood (train): ", paste(levels(train_final$Neighborhood), collapse = ", "))
message("Niveles en Neighborhood (test): ", paste(levels(test_final$Neighborhood), collapse = ", "))

```

```{r stadistics, echo=FALSE}

final_vars <- list(
  numeric = c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
              "GarageArea", "FullBath", "GarageCars"),
  categorical = c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")
)

prepare_final_data <- function(data, num_vars, cat_vars) {
  data %>% 
    select(Cara, all_of(num_vars), all_of(cat_vars)) %>%
    mutate(
      across(all_of(cat_vars), ~ forcats::fct_lump_prop(as.factor(.), prop = 0.05, other_level = "OTROS")),
      Cara = as.factor(Cara)
    )
}

train_final <- prepare_final_data(train_data, final_vars$numeric, final_vars$categorical)

train_levels <- lapply(final_vars$categorical, function(var) levels(train_final[[var]]))
names(train_levels) <- final_vars$categorical

test_final <- prepare_final_data(test_data, final_vars$numeric, final_vars$categorical)

for (var in final_vars$categorical) {
  test_final[[var]] <- factor(test_final[[var]], levels = train_levels[[var]])
}

message("Niveles en Neighborhood (train): ", paste(levels(train_final$Neighborhood), collapse = ", "))
message("Niveles en Neighborhood (test): ", paste(levels(test_final$Neighborhood), collapse = ", "))

final_model <- glm(
  Cara ~ .,
  data = train_final,
  family = binomial,
  control = list(maxit = 100)
)

train_pred <- predict(final_model, train_final, type = "response") > 0.5
test_pred  <- predict(final_model, test_final, type = "response") > 0.5

train_pred_factor <- factor(ifelse(train_pred, "1", "0"), levels = c("0", "1"))
train_true <- factor(as.character(train_final$Cara), levels = c("0", "1"))

test_pred_factor <- factor(ifelse(test_pred, "1", "0"), levels = c("0", "1"))
test_true <- factor(as.character(test_final$Cara), levels = c("0", "1"))

cm_train <- confusionMatrix(table(train_pred_factor, train_true))
cm_test  <- confusionMatrix(table(test_pred_factor, test_true))

print(cm_train)
print(cm_test)


```
El rendimiento del modelo se evalúa mediante validación cruzada y los índices de desempeño resultantes (accuracy de 77,10% en la validación cruzada de 10 folds) son razonables.

El modelo se entrenó con 520 muestras y 72 predictores, presentando un AIC de 364 y una reducción significativa en la devianza residual (de 720.25 a 297.88) lo cual sugiere que la estructura del modelo ajusta la información contenida en los datos. Aunque se indican coeficientes no definidos debido a singularidades (lo que refuerza la existencia de colinealidad en algunos predictores), la evaluación global a través de la validación cruzada (Accuracy y Kappa) proporciona evidencia de que el modelo posee una capacidad moderada a buena para discriminar entre las dos clases ("0" y "1").

Finalmente las matrices de confusión tanto para entrenamiento como para prueba muestran altos índices de exactitud (93,14% y 91,62% respectivamente), alta sensibilidad (alrededor del 94%) y especificidad también elevada (89% y 87,3% respectivamente) para la clasificación de viviendas. Estos resultados, junto con un Kappa por encima de 0.82, confirman que, a nivel predictivo, el modelo es robusto. 
```{r grapghics, echo=FALSE}


set.seed(1234)

train_sizes <- seq(0.1, 1.0, by = 0.1)

train_errors <- numeric(length(train_sizes))
val_errors   <- numeric(length(train_sizes))

for(i in seq_along(train_sizes)) {
  pct <- train_sizes[i]
  n_subset <- floor(pct * nrow(train_final))
  subset_idx <- sample(1:nrow(train_final), size = n_subset)
  train_subset <- train_final[subset_idx, ]

  model_subset <- glm(Cara ~ ., data = train_subset, family = binomial, control = list(maxit = 100))

  pred_train <- predict(model_subset, train_subset, type = "response")
  pred_train_class <- ifelse(pred_train > 0.5, 1, 0)
  # Calcular error: tasa de clasificación errónea en el entrenamiento
  train_errors[i] <- mean(pred_train_class != as.numeric(as.character(train_subset$Cara)))

  pred_val <- predict(model_subset, test_final, type = "response")
  pred_val_class <- ifelse(pred_val > 0.5, 1, 0)
  # Calcular error en validación
  val_errors[i] <- mean(pred_val_class != as.numeric(as.character(test_final$Cara)))
}

learning_curve <- data.frame(
  TrainingSize = train_sizes * 100,  # en porcentaje
  TrainingError = train_errors,
  ValidationError = val_errors
)

learning_curve_melted <- melt(learning_curve, id.vars = "TrainingSize", 
                              variable.name = "Dataset", value.name = "Error")

 ggplot(learning_curve_melted, aes(x = TrainingSize, y = Error, color = Dataset)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "Curvas de Aprendizaje del Modelo de Regresión Logística",
       x = "Porcentaje de Datos de Entrenamiento Utilizados",
       y = "Tasa de Error (Clasificación Incorrecta)") +
  theme_minimal()

```
La gráfica ilustra la evolución de la tasa de error (eje vertical) para el conjunto de entrenamiento (línea roja) y el de validación (línea azul), conforme se incrementa el porcentaje de datos de entrenamiento utilizados (eje horizontal). En los primeros tramos (menores al 30% de datos) el modelo tiende a sobreajustar(training error muy bajo) mientras que el error en validación es más alto debido a que con pocos datos el modelo se adapta excesivamente a la muestra pequeña y no generaliza tan bien. Sin embargo, a medida que aumenta el tamaño de la muestra de entrenamiento ambos errores convergen y se estabilizan, lo cual es una señal de que el modelo no sufre de sobreajuste severo cuando se dispone de suficientes datos. La brecha relativamente estrecha entre los errores de entrenamiento y validación en los últimos tramos (acercándose al 100% de datos) sugiere que el modelo encuentra un buen balance entre complejidad y capacidad de generalización. Este comportamiento respalda la confiabilidad del modelo y confirma que agregar más información de entrenamiento contribuye a una mayor robustez en el desempeño.

7.  Haga un tuneo del modelo para determinar los mejores parámetros, recuerde que los modelos de regresión logística se pueden regularizar como los de regresión lineal.


```{r tunin_model, echo=FALSE}
numeric_vars <- c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
                  "GarageArea", "FullBath", "GarageCars")
categorical_vars <- c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")

# Preparar datos
train_final <- train_data %>%
  select(Cara, all_of(numeric_vars), all_of(categorical_vars)) %>%
  mutate(across(all_of(categorical_vars), as.factor))

x_train <- model.matrix(Cara ~ ., data = train_final)[, -1]  
y_train <- train_final$Cara


test_final2 <- test_data %>%
  select(Cara, all_of(numeric_vars), all_of(categorical_vars)) %>%
  mutate(across(all_of(categorical_vars), ~ factor(., levels = levels(train_final[[cur_column()]]))))

x_test <- model.matrix(Cara ~ ., data = test_final2)[, -1]
y_test <- test_final2$Cara


set.seed(123)
alphas <- seq(0, 1, by = 0.1)  

# Función para encontrar mejor lambda y alpha
tune_glmnet <- function(alpha) {
  cv_model <- cv.glmnet(x_train, y_train, 
                        family = "binomial", 
                        alpha = alpha,
                        type.measure = "class",  # Para accuracy
                        nfolds = 10)
  data.frame(alpha = alpha,
             lambda_min = cv_model$lambda.min,
             lambda_1se = cv_model$lambda.1se,
             accuracy = 1 - min(cv_model$cvm))
}


tune_results <- map_df(alphas, tune_glmnet)


best_alpha <- tune_results$alpha[which.max(tune_results$accuracy)]
best_lambda <- tune_results$lambda_min[which.max(tune_results$accuracy)]

# Modelo final con mejor alpha y lambda
final_glmnet <- glmnet(x_train, y_train, 
                       family = "binomial",
                       alpha = best_alpha,
                       lambda = best_lambda)

# Coeficientes del modelo
coef(final_glmnet)

# Predicciones
pred_probs <- predict(final_glmnet, newx = x_test, type = "response")
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

print(paste("Mejor alpha (mezcla Ridge/Lasso):", best_alpha))
print(paste("Mejor lambda (fuerza de regularización):", best_lambda))


```
8. Haga un análisis de la eficiencia del algoritmo usando una matriz de confusión. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores, el tiempo y la memoria consumida. Para esto último puede usar “profvis” si trabaja con R

```{r metric_tunin, echo=FALSE}

# Matriz conf
conf_mat <- confusionMatrix(factor(pred_class, levels = c(0, 1)), 
                           factor(y_test, levels = c(0, 1)))
print(conf_mat)

# metricas especificas
accuracy <- conf_mat$overall["Accuracy"]
sensitivity <- conf_mat$byClass["Sensitivity"]  # Recall para clase 0 (no cara)
specificity <- conf_mat$byClass["Specificity"]  # Recall para clase 1 (cara)
precision <- conf_mat$byClass["Pos Pred Value"]  # Precisión para clase 1
f1_score <- 2 * (precision * sensitivity) / (precision + sensitivity)

cat("\n=== MÉTRICAS CLAVE ===",
    "\nAccuracy (Exactitud):", round(accuracy, 4),
    "\nSensibilidad (Clase 0 - 'No Cara'):", round(sensitivity, 4),
    "\nEspecificidad (Clase 1 - 'Cara'):", round(specificity, 4),
    "\nPrecisión (Clase 1):", round(precision, 4),
    "\nF1-Score (Balanceado):", round(f1_score, 4))


conf_df <- as.data.frame(conf_mat$table)
colnames(conf_df) <- c("Predicción", "Real", "Frecuencia")

# Heatmap de errores
ggplot(conf_df, aes(x = Real, y = Predicción, fill = Frecuencia)) +
  geom_tile() +
  geom_text(aes(label = Frecuencia), color = "white") +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Matriz de Confusión: Predicción vs Real",
       subtitle = paste("Accuracy:", round(accuracy, 2))) +
  theme_minimal()

# Perfilado del entrenamiento del modelo final
profvis({
  final_glmnet <- glmnet(x_train, y_train, 
                         family = "binomial",
                         alpha = best_alpha,
                         lambda = best_lambda)
  pred_probs <- predict(final_glmnet, newx = x_test, type = "response")
})
```

9. Determine cual de todos los modelos es mejor, puede usar AIC y BIC para esto, además de los parámetros de la matriz de confusión y los del profiler

```{r comparison_part2, echo=FALSE}
# Función para extraer métricas de una matriz de conf
get_metrics_from_cm <- function(conf_matrix) {
  data.frame(
    Accuracy = conf_matrix$overall["Accuracy"],
    Sensitivity = conf_matrix$byClass["Sensitivity"],
    Specificity = conf_matrix$byClass["Specificity"],
    F1 = conf_matrix$byClass["F1"]
  )
}

# Extraer métricas de las matrices que ya tenemos de antes
metrics_modelo_2 <- get_metrics_from_cm(cm_test)   # Modelo con selección de variables (GLM)
metrics_modelo_3 <- get_metrics_from_cm(conf_mat)  # Modelo con tuning (GLMNet)

# Crear tabla comparativa
comparison <- data.frame(
  Modelo = c("Modelo 2 (GLM)", "Modelo 3 (GLMNet)"),
  AIC = c(AIC(final_model), NA),  
  BIC = c(BIC(final_model), NA),  
  Accuracy = c(metrics_modelo_2$Accuracy, metrics_modelo_3$Accuracy),
  Sensitivity = c(metrics_modelo_2$Sensitivity, metrics_modelo_3$Sensitivity),
  Specificity = c(metrics_modelo_2$Specificity, metrics_modelo_3$Specificity),
  F1 = c(metrics_modelo_2$F1, metrics_modelo_3$F1)
)

# Añadir AIC/BIC aproximado para glmnet en caso sea necsario
if (!is.null(final_glmnet)) {
  log_lik <- sum(dbinom(y_test, 
                       size = 1, 
                       prob = predict(final_glmnet, newx = x_test, type = "response"), 
                       log = TRUE))
  k <- sum(coef(final_glmnet) != 0)  
  n <- length(y_test)
  
  comparison$AIC[2] <- -2 * log_lik + 2 * k
  comparison$BIC[2] <- -2 * log_lik + k * log(n)
}

print(comparison)

```



10. Haga un modelo de regresión logística para la variable categórica para el precio de las casas (categorías:
barata, media y cara). Asegúrese de tunearlo para obtener el mejor modelo posible.

```{r clasification_logisti_regresion, echo=FALSE}
head(train_data)
clasificaciones <- table(train_data$CategoriaPrecio)
print(clasificaciones)

train_data$CategoriaPrecio <- factor(train_data$CategoriaPrecio, 
                                    levels = c("barata", "estandar", "cara"))
test_data$CategoriaPrecio <- factor(test_data$CategoriaPrecio, 
                                   levels = levels(train_data$CategoriaPrecio))

# Seleccionar variables (usaremos las mismas del ejercicio anterior)
numeric_vars <- c("GrLivArea", "X1stFlrSF", "OverallQual", "YearBuilt", 
                  "GarageArea", "FullBath", "GarageCars")
categorical_vars <- c("Neighborhood", "BldgType", "HouseStyle", "MSZoning", "LotShape")

# Crear fórmula
formula <- as.formula(paste("CategoriaPrecio ~", 
                           paste(c(numeric_vars, categorical_vars), collapse = " + ")))

# Preparar matrices para glmnet
x_train <- model.matrix(formula, data = train_data)[, -1]  # Eliminar intercept
y_train <- train_data$CategoriaPrecio

test_complete <- test_data[complete.cases(test_data[, all.vars(formula)]), ]
y_test_complete <- test_complete$CategoriaPrecio

x_test <- model.matrix(formula, data = test_complete)[, -1]


# Tuneo de alpha (mezcla) y lambda (penalización) con validación cruzada
set.seed(123)
cv_multinom <- cv.glmnet(x_train, y_train, 
                        family = "multinomial",  # Para >2 clases
                        alpha = 0.5,             # Elastic Net (0.5 = balance)
                        type.measure = "class",  # Métrica: error de clasificación
                        nfolds = 10)

# Mejores parámetros
best_lambda <- cv_multinom$lambda.min
cat("Mejor lambda:", best_lambda, "\n")

# Modelo final con mejores parámetros
final_multinom <- glmnet(x_train, y_train, 
                        family = "multinomial",
                        alpha = 0.5,
                        lambda = best_lambda)

# Coeficientes (por categoría)
# print(coef(final_multinom))

# Predicciones en test
pred_class_multi <- predict(final_multinom, newx = x_test, type = "class")

cat("Longitud de pred_class_multi:", length(pred_class_multi), "\n")
cat("Longitud de y_test:", length(y_test_complete), "\n")

# Asegurar que los niveles de referencia y predicción coincidan
common_levels <- intersect(levels(y_test_complete), unique(pred_class_multi))

# Filtrar solo las clases presentes
y_test_filtered <- factor(y_test_complete, levels = common_levels)
pred_class_filtered <- factor(pred_class_multi, levels = common_levels)

# Verificar niveles
cat("Niveles en y_test:", levels(y_test_filtered), "\n")
cat("Niveles en predicciones:", levels(pred_class_filtered), "\n")

# Matriz de confusión corregida
conf_mat_tuned <- confusionMatrix(pred_class_filtered, y_test_filtered)
print(conf_mat_tuned)
# Métricas por clase
cat("Accuracy:", conf_mat_tuned$overall["Accuracy"], "\n")
print(conf_mat_tuned$byClass[, c("Sensitivity", "Specificity", "F1")])

```

11. Compare la eficiencia del modelo anterior con los de clasificación de las entregas anteriores ¿Cuál se demoró más en procesar?¿Cuál se equivocó más?¿Cuál se equivocó menos?¿por qué?

Esto se respinderá mejor en el documento final, ya que colocar los resultados de los módelos junto con el código sería más tardado, largo y redundante. Pero el análisis completo se encontrará en el documento final de entrega.