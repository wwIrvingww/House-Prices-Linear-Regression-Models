---
title: "Naive Bayes para clasificacion"
author: "Irving, Chuy"
date: "2025-03-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(caret)
library(readr)
library(dplyr)
library(forcats)
library(knitr)
library(e1071)
library(recipes)

```

```{r load_data, include=FALSE}
train_data <- read.csv("train_final.csv")
test_data <- read.csv("test_final.csv")

```

```{r remove_na_columns, include=FALSE}
porcentaje_na <- sapply(train_data, function(x) sum(is.na(x)) / length(x)) * 100

columnas_a_eliminar <- names(porcentaje_na[porcentaje_na > 75])
print(columnas_a_eliminar)

train_data <- train_data[, !names(train_data) %in% columnas_a_eliminar]
test_data <- test_data[, !names(test_data) %in% columnas_a_eliminar]

# Combinar conjuntos de entrenamiento y prueba
combined_data <- bind_rows(train_data, test_data)

# Asegurar que los niveles de las variables categóricas sean consistentes
combined_data <- combined_data %>% mutate(across(where(is.factor), ~ fct_unify(list(.))))

# Dividir nuevamente en conjuntos de entrenamiento y prueba
train_data <- combined_data[1:nrow(train_data), ]
test_data <- combined_data[(nrow(train_data) + 1):nrow(combined_data), ]

```


## 4. Clasificacion de variables SalePrice

Se realiza una clasificación para saber si una casa tiene un precio barato, medio o caro. Para esto hay que hacer una variable categórica que tenga 3 categorías barato, medio y caro.
```{r discretizacion, include=FALSE}

categorias <- cut(train_data$SalePrice, breaks=c(min(train_data$SalePrice),
                                                  quantile(train_data$SalePrice, probs = 1/3),
                                                  quantile(train_data$SalePrice, probs = 2/3),
                                                  max(train_data$SalePrice)),
                  labels=c("barata", "estandar", "cara"), include.lowest=TRUE)

train_data$consumoCat <- categorias

categorias_test <- cut(test_data$SalePrice, 
                       breaks=c(min(test_data$SalePrice),
                                quantile(test_data$SalePrice, probs = 1/3),
                                quantile(test_data$SalePrice, probs = 2/3),
                                max(test_data$SalePrice)),
                       labels=c("barata", "estandar", "cara"), include.lowest=TRUE)
test_data$consumoCat <- categorias_test


```

Luego de discretizar la variable de precio de venta de la casa, usando los cuartiles, quedan las categorías en la siguiente proporpoción

## puntos de corte
```{r cortes, echo=FALSE}
puntos_de_corte <- quantile(train_data$SalePrice, probs = c(0, 1/3, 2/3, 1))

print(puntos_de_corte)

```

```{r variable_categrica}
table(train_data$consumoCat)
#quitar la variable respuesta anteriore del precio
train_data <- train_data %>% select(-SalePrice)
# quitar columna SalePrice de test_data 
test_data <- test_data %>% select(-SalePrice)

# Entrenar el modelo
model <- naiveBayes(consumoCat ~ ., data = train_data)

```
Lo que hicimos fue discretizar la variable resspuesta de precio con cuantiles. Es prácticamente dividir los datos en distintos rangos de precios (en porcentajes por los cuantiles) de casas para poder convertir la variable cuantitativa continua en en una categórica. Esto ayudará a poder hacer predicciones y evaluar el modelo con una matriz de confusión. Específicamente partimos la información en 3 cuantiles para poder hacer la clasificación las casas baratas estan en precios por debajo del percentil 33, las casas Estándar se encuentran entre el percentil 66 y las casas caras se encuentran sobre el percentil de 66. Con esto tuvimos resultados bastante buenos y los datos quedaron bastante balanceados. Habiedno **341 casas baratas**, **346 estándar** y **334 caras**. El balanceo de datos es muy importante ya que es necesario para que no hayan sesgos en la prueba del accuracy.

## 5. Utilice los modelos con el conjunto de prueba y determine la eficiencia del algoritmo para predecir y clasificar. 

```{r predict_and_test}
# Realizar predicciones en el conjunto de prueba
predictions <- predict(model, test_data)

conf_matrix <- confusionMatrix(predictions, test_data$consumoCat)
print(conf_matrix)

# Precisión general
accuracy <- conf_matrix$overall['Accuracy']
print(paste("Precisión general:", accuracy))

# Precisión, recall y F1-score por clase
precision <- conf_matrix$byClass[, "Precision"]
recall <- conf_matrix$byClass[, "Recall"]
f1_score <- conf_matrix$byClass[, "F1"]

print("Precisión por clase:")
print(precision)

print("Recall por clase:")
print(recall)

print("F1-score por clase:")
print(f1_score)

```

Al final hemos obtenido un accuracy general de 67.2%, no es el mejor modelo de predicción aún pero es mejor que una clasificación aleatoria lo cuál es un buen indicador. Nuevamente el modelo puede que esté teniendo algunos problemas para clasificar por la gran cantidad de variables, ya que solo removimos  las columnas que tuvieran demasiados valores vacíos. Pero aparte de eso se incluyeron todas las variables tanto categ´ricas como numéricas. 

Para tener un buen entendimiento de los resultados se realizaron las pruebas de accuracy, recall y f1 score para cada una de las clasificaciones aparte de la evaluación general. Donde se puede ver que realmente el mayor problemas es la clasificación de casa **Estándar**, siendo la que tiene las peores métricas con diferencia en todas las evaluaciones. El accuracy fue del 60% y el recall de 26%, lo cuál indica que la clasificación realmente es mala en la mayoría de la casas para las casos de precio "promedio" en base a sus características. Para profundizar más en estos resultados es útil también visualizar la matríz de confusión. 

## 6. Haga un análisis de la eficiencia del modelo de clasificación usando una matriz de confusión. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores.

```{r confusion_matrix}
# Evaluar el modelo
conf_matrix <- confusionMatrix(predictions, test_data$consumoCat)
print(conf_matrix)

ggplot(as.data.frame(conf_matrix$table), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Matriz de Confusión",
       x = "Valor Real",
       y = "Predicción") +
  theme_minimal()

```


La matríz al final nos confirma el análisis realizado en el inciso anterior, realmente la clasificación con más problemas es la estándar. Pero ahora con la matríz podemos ver como es que esta clasificación se está realizando. Resulta que la clase estándar por lo general se clasifica de manera erronea como barata. Realmente fueron pocas las casas estándar que fueron calificadas como caras. Viendo la diagonal, en este caso de esquina inferior izquierda a esquna superior derecha por la posición de los labels en el gráfico, realmente la clasificación de casas baratas y caras fueron bastante exitosas y tienen un error pequeño. El color azul fuerte junto con el número nos lo confirma (agregado a las métricas analizidas anteriormente). La matriz es muy importante para analizar errores de manera visual y conocer específicamente en que parte es que el modelo falló y como falló. Ahora lo que corresponde es mejorar el módelo ajustando variables, eliminando variables o bien tuneando hiperparámetros para mejorar el desempeño. 

# 7 Analice el modelo. ¿Cree que pueda estar sobreajustado?

Ahora bien aunque los resultados fueron buenos para 2 de las 3 posibles claisificaciones, hay que tener cuidado de que no haya overfitting. Para poder evaluar si los datos están sobre ajustados podemos probar a hacer no solo predicciones con el set de datos de prueba, sino también el mismo set de datos de entrenamiento. Si los resultados de hacer predicciones con el mismo set de entrenamiento resultan ser mejores, quiere decir que el modelo no está generalizando correctamente y tiene tendencia a clasificar mejor los datos parecidos al set de de datos de entrenamiento.

``` {r has_overfitting? }
#accuracy conjunto de entrenamiento
train_predictions <- predict(model, train_data)
train_accuracy <- sum(train_predictions == train_data$consumoCat) / length(train_predictions)
print(paste("Precisión en entrenamiento:", train_accuracy))

# accuracy conjunto de prueba
test_accuracy <- conf_matrix$overall['Accuracy']
print(paste("Precisión en prueba:", test_accuracy))
```

Como se puede ver al final realmente la diferencia entre las predicciones usando el set de testing y el set de entrenamiento no varía casi nada. Esto quiere decir que el modelo realmente no tiene overfitting y con datos distintos da resultados bastante similares. El modelo no tiene sobreajuste pero seguramente hubiera sido necesario hacer más procesamiento de las variables para que fuera mejor clasificando los 3 distintos tipos de casa y no solo 2 (las caras y las baratas).

##Usando validacion cruzada
Haga un modelo usando validación cruzada, compare los resultados de este con los del
modelo anterior. ¿Cuál funcionó mejor?


```{r, include=FALSE}
ctrl <- trainControl(method = "cv",
                     number = 10,
                     savePredictions = "final",
                     classProbs = TRUE,
                     summaryFunction = defaultSummary)

print("Control de validación cruzada configurado con éxito.")
```

```{r, include=FALSE}
train_data <- read.csv("train_final.csv")
test_data <- read.csv("test_final.csv")

print(head(train_data))
print(head(test_data))

print(summary(train_data))
print(summary(test_data))

if("YourCategoricalVariable" %in% names(train_data) && any(!is.na(train_data$YourCategoricalVariable))) {
    train_data$YourCategoricalVariable <- factor(train_data$YourCategoricalVariable)
}
if("YourCategoricalVariable" %in% names(test_data) && any(!is.na(test_data$YourCategoricalVariable))) {
    test_data$YourCategoricalVariable <- factor(test_data$YourCategoricalVariable)
}

if("YourNumericVariable" %in% names(train_data)) {
    train_data$YourNumericVariable <- ifelse(is.na(train_data$YourNumericVariable), 
                                             mean(train_data$YourNumericVariable, na.rm = TRUE), 
                                             train_data$YourNumericVariable)
}

print(str(train_data))
print(str(test_data))


```

```{r echo=FALSE}
data_recipe <- recipe(~ ., data = train_data) %>%
  # Manejar nuevos niveles en variables categóricas
  step_novel(all_nominal(), -all_outcomes()) %>%
  # Eliminar variables con variación cero
  step_zv(all_predictors()) %>%
  # Imputación de valores NA para variables numéricas
  step_impute_median(all_numeric(), -all_outcomes()) %>%
  # Imputación de valores NA para variables categóricas
  step_impute_mode(all_nominal(), -all_outcomes()) %>%
  # Crear dummies para variables categóricas
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  # Escalar y centrar variables numéricas
  step_scale(all_numeric()) %>%
  step_center(all_numeric())

prepped_data <- prep(data_recipe, training = train_data)

train_data_transformed <- bake(prepped_data, new_data = NULL)

test_data_transformed <- bake(prepped_data, new_data = test_data)

print(str(train_data_transformed))
print(str(test_data_transformed))

```

```{r echo=FALSE}
train_data$consumoCat <- cut(train_data$SalePrice,
                             breaks = breaks,
                             labels = c("barata", "estandar", "cara"),
                             include.lowest = TRUE)
train_data$consumoCat <- as.factor(train_data$consumoCat)

test_data$consumoCat <- cut(test_data$SalePrice,
                            breaks = breaks,
                            labels = c("barata", "estandar", "cara"),
                            include.lowest = TRUE)
test_data$consumoCat <- as.factor(test_data$consumoCat)

if(!identical(levels(train_data$consumoCat), levels(test_data$consumoCat))) {
  all_levels <- union(levels(train_data$consumoCat), levels(test_data$consumoCat))
  train_data$consumoCat <- factor(train_data$consumoCat, levels = all_levels)
  test_data$consumoCat <- factor(test_data$consumoCat, levels = all_levels)
}

prepped_data <- prep(data_recipe, training = train_data)

train_data_transformed <- bake(prepped_data, new_data = NULL)

test_data_transformed <- bake(prepped_data, new_data = test_data)

train_data_transformed$consumoCat <- train_data$consumoCat
test_data_transformed$consumoCat <- test_data$consumoCat

set.seed(123)
model_nb <- train(
  consumoCat ~ .,
  data = train_data_transformed,
  method = "naive_bayes",
  trControl = train_control,
  metric = "Accuracy"
)


print(model_nb)
```


```{r}

predictions <- predict(model_nb, newdata = test_data_transformed)

conf_matrix <- confusionMatrix(predictions, test_data_transformed$consumoCat)

print(conf_matrix)

plot(conf_matrix$table, col = conf_matrix$byClass, main = "Matriz de Confusión")

accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
sensitivity <- conf_matrix$byClass['Sensitivity']
specificity <- conf_matrix$byClass['Specificity']

cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")


```
Los resultados de la matriz de confusión y las estadísticas del modelo Naive Byaes son bastantes decepcionantes, el peor de todos los modelos hasta ahora.  
Su *precisión* es del *55.55%*, lo cual es ligeramente mejor que el azar, ya que la taza del *No information Rate* es de *33.94%*.  
En cuanto a la sensibilidad y especifidad es muy alta para la clase `cara` con un valor de *95.3%*, pero es extremedamante baja para la clase `barata`, con un 0. Lo que nos dice que el modelo clasifica correctamente las casas caras, pero falla por completo al identificar las baratas.  
El valor predicitivo positivo puede considerarse alto para la clase `cara` con un valor de 76.34%  lo que indica que cuando el modelo predice que una vivienda es cara, es probable que así sea. Sin embargo, el VPP para la clase `estandar` es de solo 40.32%, lo que muestra una eficiencia limitada en estas predicciones.  
La ausencia de predicciones para la clase "barata" sugiere que el modelo puede estar sesgado hacia las otras dos categorías, posiblemente debido a un desbalance en los datos de entrenamiento.  
El valaor *kappa de 0.3342* sugiere que existe mucho margen de mejora.

##¿Cuál es mejor?
El primero modelo (el anterior) muestra un rendimiento superior en términos de precisión general y balance entre sensibilidad y especificidad en comparación con el de validación cruzada.  
El de validación cruzada falló por completo al detectar la categoría `barata`, mientras que el segundo modelo obtuvo una sensibilidad de 99.34% casi perecta, lo cual también es un poco preocupante.  
El balance Acurracy en el modelo anteior es superior con un 82.31% para la clase `barata` y 85.37 para `cara` lo que demuestra un equilibrio a la hora de detectar verdaeres positvos y negativos por categoria.  
El valor de kappa en el modelo anterior fue de 0.5047 sugiera ser mejor que la aleatoriedad y es además superior a la del modelo de validación cruzada que apenas alcanzó un 0.3342

##Tunning
```{r nan_values, include=FALSE}
threshold <- 0.80
cols_to_remove <- sapply(train_data, function(x) mean(is.na(x))) > threshold
cols_to_remove <- names(cols_to_remove[cols_to_remove])

train_data <- train_data %>% select(-one_of(cols_to_remove))
test_data <- test_data %>% select(-one_of(cols_to_remove))

train_data <- train_data %>%
  mutate_if(is.numeric, ~ifelse(is.na(.), median(., na.rm = TRUE), .))

train_data <- train_data %>%
  mutate_if(is.factor, ~ifelse(is.na(.), as.character(get_mode(.)), .))

get_mode <- function(v) {
  uniqv <- unique(na.omit(v))
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

sum_na_clean <- sapply(train_data, function(x) sum(is.na(x)))

head(train_data)

train_data <- na.omit(train_data)
test_data <- na.omit(test_data)


```

```{r}
multiClassSummary <- function(data, lev = NULL, model = NULL) {
  cm <- confusionMatrix(data$pred, data$obs, dnn = c('Prediction', 'Reference'))
  accuracy <- cm$overall['Accuracy']

  # Evitar NaN cuando no hay predicciones para una clase
  precision <- ifelse(is.nan(cm$byClass['Precision']), 0, cm$byClass['Precision'])
  recall <- ifelse(is.nan(cm$byClass['Recall']), 0, cm$byClass['Recall'])
  F1 <- ifelse(is.nan(cm$byClass['F1']), 0, cm$byClass['F1'])

  # Devuelve un listado con todas las métricas
  c(Accuracy = accuracy, Precision = mean(precision, na.rm = TRUE), Recall = mean(recall, na.rm = TRUE), F1 = mean(F1, na.rm = TRUE))
}
table(train_data_transformed$consumoCat)


```


```{r}
# Cargar las bibliotecas necesarias
library(caret)
library(doParallel)

# Configurar el control de entrenamiento con paralelización y menos repeticiones
registerDoParallel(cores = detectCores() - 1)  # Utiliza todos los núcleos disponibles menos uno

# Definir una función de resumen personalizada para clasificación multiclase
multiClassSummary <- function(data, lev = NULL, model = NULL) {
  if (length(lev) != 3) stop("Your outcome has to have exactly three levels!")
  
  # Calcula la matriz de confusión y extrae las métricas
  cm <- confusionMatrix(data$pred, data$obs)
  accuracy <- cm$overall['Accuracy']
  precision <- cm$byClass['Precision']
  recall <- cm$byClass['Recall']
  F1 <- cm$byClass['F1']
  
  # Promedio de Precision, Recall y F1 si se requiere manejar múltiples clases
  mean_precision <- mean(precision, na.rm = TRUE)
  mean_recall <- mean(recall, na.rm = TRUE)
  mean_F1 <- mean(F1, na.rm = TRUE)
  
  # Devuelve un listado con todas las métricas
  c(Accuracy = accuracy, Precision = mean_precision, Recall = mean_recall, F1 = mean_F1)
}

# Configuración del control de entrenamiento
trainControl <- trainControl(
  method = "cv",
  number = 5,  # Reducción de pliegues
  savePredictions = "final",
  summaryFunction = multiClassSummary,
  allowParallel = TRUE  # Permite la paralelización
)

# Rejilla de tunning simplificada
tuneGrid <- expand.grid(
  laplace = c(0, 1),  # Valores críticos
  usekernel = c(TRUE, FALSE),
  adjust = c(1)  # Valor central
)

# Asumiendo que 'train_data_transformed' es tu conjunto de datos ya procesado y listo para usar
# Entrenar el modelo con la nueva configuración de resumen
set.seed(123)  # Para asegurar reproducibilidad
modelo_nb_tuned <- train(
  consumoCat ~ .,
  data = train_data_transformed,
  method = "naive_bayes",
  trControl = trainControl,
  tuneGrid = tuneGrid,
  metric = "Accuracy"
)

# Imprimir los resultados del modelo tuneado
print(modelo_nb_tuned)

```


```{r}
library(caret)

# Definir una función de resumen personalizada para clasificación multiclase
multiClassSummary <- function(data, lev = NULL, model = NULL) {
  if (is.null(lev)) lev <- levels(data$obs)
  if (length(lev) != 3) stop("Your outcome has to have exactly three levels!")
  
  # Calcula la matriz de confusión
  cm <- confusionMatrix(data$pred, data$obs, mode = "prec_recall")

  # Extraer métricas
  precision <- cm$byClass['Precision']
  recall <- cm$byClass['Recall']
  F1 <- cm$byClass['F1']
  accuracy <- cm$overall['Accuracy']
  
  # Devolver todas las métricas
  c(Accuracy = accuracy, Precision = mean(precision, na.rm = TRUE), Recall = mean(recall, na.rm = TRUE), F1 = mean(F1, na.rm = TRUE))
}

# Configuración del control de entrenamiento
trainControl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  summaryFunction = multiClassSummary,
  classProbs = TRUE  # Si necesitas probabilidades de clase
)

# Configuración de la rejilla de tunning
tuneGrid <- expand.grid(
  laplace = seq(0, 5, by = 1),
  usekernel = c(TRUE, FALSE),
  adjust = seq(0.5, 2, by = 0.5)
)

# Entrenar el modelo con la nueva función de resumen
set.seed(123)
model_nb_tuned <- train(
  consumoCat ~ .,
  data = train_data_transformed,
  method = "naive_bayes",
  trControl = trainControl,
  tuneGrid = tuneGrid,
  metric = "Accuracy"
)

# Imprimir los resultados del modelo tuneado
print(model_nb_tuned)

```

```{r}
library(caret)

# Definir una función de resumen personalizada usando funciones disponibles en caret
multiClassSummaryCustom <- function(data, lev = NULL, model = NULL) {
  # ConfMatrix es una matriz de confusión detallada
  confMatrix <- confusionMatrix(data$pred, data$obs, mode = "everything", dnn = c("Prediction", "Reference"))

  # Extraer las métricas de precisión, recall y F1 de la matriz de confusión
  precision <- confMatrix$byClass['Precision']
  recall <- confMatrix$byClass['Recall']
  F1 <- 2 * (precision * recall) / (precision + recall)  # Calculando F1-score

  # Calculando la precisión global
  accuracy <- sum(diag(confMatrix$table)) / sum(confMatrix$table)
  
  out <- c(accuracy, precision, recall, F1)
  names(out) <- c("Accuracy", "Precision", "Recall", "F1")
  out
}

# Configurar el control de entrenamiento
trainControl <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE,  # Solo necesario si planeas usar las probabilidades de clase
  summaryFunction = multiClassSummaryCustom
)

# Configuración de la rejilla de tunning
tuneGrid <- expand.grid(
  laplace = seq(0, 5, by = 1),
  usekernel = c(TRUE, FALSE),
  adjust = seq(0.5, 2, by = 0.5)
)

set.seed(123)
modelo_nb_tuned <- train(
  consumoCat ~ .,
  data = train_data_transformed,
  method = "naive_bayes",
  trControl = trainControl,
  tuneGrid = tuneGrid,
  metric = "Accuracy"
)

# Mostrar los resultados del tunning
print(modelo_nb_tuned)


```