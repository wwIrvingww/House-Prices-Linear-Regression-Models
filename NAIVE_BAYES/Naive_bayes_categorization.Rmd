---
title: "Naive Bayes para clasificacion"
author: "Irving, Chuy"
date: "2025-03-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(caret)
library(readr)
library(dplyr)
library(forcats)
library(knitr)
library(e1071)
library(recipes)

```

```{r load_data, include=FALSE}
train_data <- read.csv("train_final.csv")
test_data <- read.csv("test_final.csv")

```

```{r remove_na_columns, include=FALSE}
porcentaje_na <- sapply(train_data, function(x) sum(is.na(x)) / length(x)) * 100

columnas_a_eliminar <- names(porcentaje_na[porcentaje_na > 75])
print(columnas_a_eliminar)

train_data <- train_data[, !names(train_data) %in% columnas_a_eliminar]
test_data <- test_data[, !names(test_data) %in% columnas_a_eliminar]

# Combinar conjuntos de entrenamiento y prueba
combined_data <- bind_rows(train_data, test_data)

# Asegurar que los niveles de las variables categóricas sean consistentes
combined_data <- combined_data %>% mutate(across(where(is.factor), ~ fct_unify(list(.))))

# Dividir nuevamente en conjuntos de entrenamiento y prueba
train_data <- combined_data[1:nrow(train_data), ]
test_data <- combined_data[(nrow(train_data) + 1):nrow(combined_data), ]

```


## 4. Clasificacion de variables SalePrice

Se realiza una clasificación para saber si una casa tiene un precio barato, medio o caro. Para esto hay que hacer una variable categórica que tenga 3 categorías barato, medio y caro.
```{r discretizacion, include=FALSE}

categorias <- cut(train_data$SalePrice, breaks=c(min(train_data$SalePrice),
                                                  quantile(train_data$SalePrice, probs = 1/3),
                                                  quantile(train_data$SalePrice, probs = 2/3),
                                                  max(train_data$SalePrice)),
                  labels=c("barata", "estandar", "cara"), include.lowest=TRUE)

train_data$consumoCat <- categorias

categorias_test <- cut(test_data$SalePrice, 
                       breaks=c(min(test_data$SalePrice),
                                quantile(test_data$SalePrice, probs = 1/3),
                                quantile(test_data$SalePrice, probs = 2/3),
                                max(test_data$SalePrice)),
                       labels=c("barata", "estandar", "cara"), include.lowest=TRUE)
test_data$consumoCat <- categorias_test


```

Luego de discretizar la variable de precio de venta de la casa, usando los cuartiles, quedan las categorías en la siguiente proporpoción

## puntos de corte
```{r cortes, echo=FALSE}
puntos_de_corte <- quantile(train_data$SalePrice, probs = c(0, 1/3, 2/3, 1))

print(puntos_de_corte)

```

```{r variable_categrica, echo=FALSE}
table(train_data$consumoCat)
#quitar la variable respuesta anteriore del precio
train_data <- train_data %>% select(-SalePrice)
# quitar columna SalePrice de test_data 
test_data <- test_data %>% select(-SalePrice)

# Entrenar el modelo
model <- naiveBayes(consumoCat ~ ., data = train_data)

```
Lo que hicimos fue discretizar la variable resspuesta de precio con cuantiles. Es prácticamente dividir los datos en distintos rangos de precios (en porcentajes por los cuantiles) de casas para poder convertir la variable cuantitativa continua en en una categórica. Esto ayudará a poder hacer predicciones y evaluar el modelo con una matriz de confusión. Específicamente partimos la información en 3 cuantiles para poder hacer la clasificación las casas baratas estan en precios por debajo del percentil 33, las casas Estándar se encuentran entre el percentil 66 y las casas caras se encuentran sobre el percentil de 66. Con esto tuvimos resultados bastante buenos y los datos quedaron bastante balanceados. Habiedno **341 casas baratas**, **346 estándar** y **334 caras**. El balanceo de datos es muy importante ya que es necesario para que no hayan sesgos en la prueba del accuracy.

## 5. Utilice los modelos con el conjunto de prueba y determine la eficiencia del algoritmo para predecir y clasificar. 

```{r predict_and_test, echo=FALSE}
# Realizar predicciones en el conjunto de prueba
predictions <- predict(model, test_data)

conf_matrix <- confusionMatrix(predictions, test_data$consumoCat)
print(conf_matrix)

# Precisión general
accuracy <- conf_matrix$overall['Accuracy']
print(paste("Precisión general:", accuracy))

# Precisión, recall y F1-score por clase
precision <- conf_matrix$byClass[, "Precision"]
recall <- conf_matrix$byClass[, "Recall"]
f1_score <- conf_matrix$byClass[, "F1"]

print("Precisión por clase:")
print(precision)

print("Recall por clase:")
print(recall)

print("F1-score por clase:")
print(f1_score)

```

Al final hemos obtenido un accuracy general de 67.2%, no es el mejor modelo de predicción aún pero es mejor que una clasificación aleatoria lo cuál es un buen indicador. Nuevamente el modelo puede que esté teniendo algunos problemas para clasificar por la gran cantidad de variables, ya que solo removimos  las columnas que tuvieran demasiados valores vacíos. Pero aparte de eso se incluyeron todas las variables tanto categ´ricas como numéricas. 

Para tener un buen entendimiento de los resultados se realizaron las pruebas de accuracy, recall y f1 score para cada una de las clasificaciones aparte de la evaluación general. Donde se puede ver que realmente el mayor problemas es la clasificación de casa **Estándar**, siendo la que tiene las peores métricas con diferencia en todas las evaluaciones. El accuracy fue del 60% y el recall de 26%, lo cuál indica que la clasificación realmente es mala en la mayoría de la casas para las casos de precio "promedio" en base a sus características. Para profundizar más en estos resultados es útil también visualizar la matríz de confusión. 

## 6. Haga un análisis de la eficiencia del modelo de clasificación usando una matriz de confusión. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores.

```{r confusion_matrix, echo=FALSE}
# Evaluar el modelo
conf_matrix <- confusionMatrix(predictions, test_data$consumoCat)
print(conf_matrix)

ggplot(as.data.frame(conf_matrix$table), aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Matriz de Confusión",
       x = "Valor Real",
       y = "Predicción") +
  theme_minimal()

```


La matríz al final nos confirma el análisis realizado en el inciso anterior, realmente la clasificación con más problemas es la estándar. Pero ahora con la matríz podemos ver como es que esta clasificación se está realizando. Resulta que la clase estándar por lo general se clasifica de manera erronea como barata. Realmente fueron pocas las casas estándar que fueron calificadas como caras. Viendo la diagonal, en este caso de esquina inferior izquierda a esquna superior derecha por la posición de los labels en el gráfico, realmente la clasificación de casas baratas y caras fueron bastante exitosas y tienen un error pequeño. El color azul fuerte junto con el número nos lo confirma (agregado a las métricas analizidas anteriormente). La matriz es muy importante para analizar errores de manera visual y conocer específicamente en que parte es que el modelo falló y como falló. Ahora lo que corresponde es mejorar el módelo ajustando variables, eliminando variables o bien tuneando hiperparámetros para mejorar el desempeño. 

# 7 Analice el modelo. ¿Cree que pueda estar sobreajustado?

Ahora bien aunque los resultados fueron buenos para 2 de las 3 posibles claisificaciones, hay que tener cuidado de que no haya overfitting. Para poder evaluar si los datos están sobre ajustados podemos probar a hacer no solo predicciones con el set de datos de prueba, sino también el mismo set de datos de entrenamiento. Si los resultados de hacer predicciones con el mismo set de entrenamiento resultan ser mejores, quiere decir que el modelo no está generalizando correctamente y tiene tendencia a clasificar mejor los datos parecidos al set de de datos de entrenamiento.

``` {r has_overfitting?, echo=FALSE }
#accuracy conjunto de entrenamiento
train_predictions <- predict(model, train_data)
train_accuracy <- sum(train_predictions == train_data$consumoCat) / length(train_predictions)
print(paste("Precisión en entrenamiento:", train_accuracy))

# accuracy conjunto de prueba
test_accuracy <- conf_matrix$overall['Accuracy']
print(paste("Precisión en prueba:", test_accuracy))
```

Como se puede ver al final realmente la diferencia entre las predicciones usando el set de testing y el set de entrenamiento no varía casi nada. Esto quiere decir que el modelo realmente no tiene overfitting y con datos distintos da resultados bastante similares. El modelo no tiene sobreajuste pero seguramente hubiera sido necesario hacer más procesamiento de las variables para que fuera mejor clasificando los 3 distintos tipos de casa y no solo 2 (las caras y las baratas).

##Usando validacion cruzada
Haga un modelo usando validación cruzada, compare los resultados de este con los del
modelo anterior. ¿Cuál funcionó mejor?


```{r configuration, include=FALSE}
ctrl <- trainControl(method = "cv",
                     number = 10,
                     savePredictions = "final",
                     classProbs = TRUE,
                     summaryFunction = defaultSummary)

print("Control de validación cruzada configurado con éxito.")
```

```{r cross, include=FALSE}
train_data <- read.csv("train_final.csv")
test_data <- read.csv("test_final.csv")

print(head(train_data))
print(head(test_data))

print(summary(train_data))
print(summary(test_data))

if("YourCategoricalVariable" %in% names(train_data) && any(!is.na(train_data$YourCategoricalVariable))) {
    train_data$YourCategoricalVariable <- factor(train_data$YourCategoricalVariable)
}
if("YourCategoricalVariable" %in% names(test_data) && any(!is.na(test_data$YourCategoricalVariable))) {
    test_data$YourCategoricalVariable <- factor(test_data$YourCategoricalVariable)
}

if("YourNumericVariable" %in% names(train_data)) {
    train_data$YourNumericVariable <- ifelse(is.na(train_data$YourNumericVariable), 
                                             mean(train_data$YourNumericVariable, na.rm = TRUE), 
                                             train_data$YourNumericVariable)
}

print(str(train_data))
print(str(test_data))


```

```{r repair_data, echo=FALSE}
data_recipe <- recipe(~ ., data = train_data) %>%
  # Manejar nuevos niveles en variables categóricas
  step_novel(all_nominal(), -all_outcomes()) %>%
  # Eliminar variables con variación cero
  step_zv(all_predictors()) %>%
  # Imputación de valores NA para variables numéricas
  step_impute_median(all_numeric(), -all_outcomes()) %>%
  # Imputación de valores NA para variables categóricas
  step_impute_mode(all_nominal(), -all_outcomes()) %>%
  # Crear dummies para variables categóricas
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) %>%
  # Escalar y centrar variables numéricas
  step_scale(all_numeric()) %>%
  step_center(all_numeric())

prepped_data <- prep(data_recipe, training = train_data)

train_data_transformed <- bake(prepped_data, new_data = NULL)

test_data_transformed <- bake(prepped_data, new_data = test_data)

print(str(train_data_transformed))
print(str(test_data_transformed))

```

```{r model, echo=FALSE}
breaks <- quantile(train_data$SalePrice, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE)
train_data$consumoCat <- cut(train_data$SalePrice,
                             breaks = breaks,
                             labels = c("barata", "estandar", "cara"),
                             include.lowest = TRUE)
train_data$consumoCat <- as.factor(train_data$consumoCat)

test_data$consumoCat <- cut(test_data$SalePrice,
                            breaks = breaks,
                            labels = c("barata", "estandar", "cara"),
                            include.lowest = TRUE)
test_data$consumoCat <- as.factor(test_data$consumoCat)

if(!identical(levels(train_data$consumoCat), levels(test_data$consumoCat))) {
  all_levels <- union(levels(train_data$consumoCat), levels(test_data$consumoCat))
  train_data$consumoCat <- factor(train_data$consumoCat, levels = all_levels)
  test_data$consumoCat <- factor(test_data$consumoCat, levels = all_levels)
}

prepped_data <- prep(data_recipe, training = train_data)

train_data_transformed <- bake(prepped_data, new_data = NULL)

test_data_transformed <- bake(prepped_data, new_data = test_data)

train_data_transformed$consumoCat <- train_data$consumoCat
test_data_transformed$consumoCat <- test_data$consumoCat

set.seed(123)
model_nb <- train(
  consumoCat ~ .,
  data = train_data_transformed,
  method = "naive_bayes",
  trControl = ctrl,
  metric = "Accuracy"
)


print(model_nb)
```


```{r testing_model, echo=FALSE}

predictions <- predict(model_nb, newdata = test_data_transformed)

conf_matrix <- confusionMatrix(predictions, test_data_transformed$consumoCat)

print(conf_matrix)

plot(conf_matrix$table, col = conf_matrix$byClass, main = "Matriz de Confusión")

accuracy <- sum(diag(conf_matrix$table)) / sum(conf_matrix$table)
sensitivity <- conf_matrix$byClass['Sensitivity']
specificity <- conf_matrix$byClass['Specificity']

cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")


```
Los resultados de la matriz de confusión y las estadísticas del modelo Naive Byaes son bastantes decepcionantes, el peor de todos los modelos hasta ahora.  
Su *precisión* es del *55.55%*, lo cual es ligeramente mejor que el azar, ya que la taza del *No information Rate* es de *33.94%*.  
En cuanto a la sensibilidad y especifidad es muy alta para la clase `cara` con un valor de *95.3%*, pero es extremedamante baja para la clase `barata`, con un 0. Lo que nos dice que el modelo clasifica correctamente las casas caras, pero falla por completo al identificar las baratas.  
El valor predicitivo positivo puede considerarse alto para la clase `cara` con un valor de 76.34%  lo que indica que cuando el modelo predice que una vivienda es cara, es probable que así sea. Sin embargo, el VPP para la clase `estandar` es de solo 40.32%, lo que muestra una eficiencia limitada en estas predicciones.  
La ausencia de predicciones para la clase "barata" sugiere que el modelo puede estar sesgado hacia las otras dos categorías, posiblemente debido a un desbalance en los datos de entrenamiento.  
El valaor *kappa de 0.3342* sugiere que existe mucho margen de mejora.

##¿Cuál es mejor?
El primero modelo (el anterior) muestra un rendimiento superior en términos de precisión general y balance entre sensibilidad y especificidad en comparación con el de validación cruzada.  
El de validación cruzada falló por completo al detectar la categoría `barata`, mientras que el segundo modelo obtuvo una sensibilidad de 99.34% casi perecta, lo cual también es un poco preocupante.  
El balance Acurracy en el modelo anteior es superior con un 82.31% para la clase `barata` y 85.37 para `cara` lo que demuestra un equilibrio a la hora de detectar verdaeres positvos y negativos por categoria.  
El valor de kappa en el modelo anterior fue de 0.5047 sugiera ser mejor que la aleatoriedad y es además superior a la del modelo de validación cruzada que apenas alcanzó un 0.3342

##Tunning

```{r nan_values, include=FALSE}
threshold <- 0.80
cols_to_remove <- sapply(train_data, function(x) mean(is.na(x))) > threshold
cols_to_remove <- names(cols_to_remove[cols_to_remove])

train_data <- train_data %>% select(-one_of(cols_to_remove))
test_data <- test_data %>% select(-one_of(cols_to_remove))

train_data <- train_data %>%
  mutate_if(is.numeric, ~ifelse(is.na(.), median(., na.rm = TRUE), .))

train_data <- train_data %>%
  mutate_if(is.factor, ~ifelse(is.na(.), as.character(get_mode(.)), .))

get_mode <- function(v) {
  uniqv <- unique(na.omit(v))
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

sum_na_clean <- sapply(train_data, function(x) sum(is.na(x)))

head(train_data)

train_data <- na.omit(train_data)
test_data <- na.omit(test_data)


```

```{r tunning_params, echo=FALSE}

laplace_values <- c(0, 0.5, 1, 2)

evaluate_naive_bayes <- function(laplace_value) {
  model <- naiveBayes(consumoCat ~ ., data = train_data, laplace = laplace_value)
  predictions <- predict(model, test_data)
  conf_matrix <- table(test_data$consumoCat, predictions)
  accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
  return(c(Laplace = laplace_value, Accuracy = accuracy))
}

results <- sapply(laplace_values, evaluate_naive_bayes)

print(data.frame(results))


```


```{r testing_tunning, echo=FALSE}


# Entrenar el modelo Naive Bayes con el valor óptimo de Laplace
optimal_laplace <- 0  # Este es el valor que encontraste como óptimo
model_nb_optimal <- naiveBayes(consumoCat ~ ., data = train_data, laplace = optimal_laplace)

# Realizar predicciones sobre el conjunto de prueba
predictions_nb_optimal <- predict(model_nb_optimal, test_data)

# Generar la matriz de confusión
conf_matrix_optimal <- table(Predicted = predictions_nb_optimal, Actual = test_data$consumoCat)

# Imprimir la matriz de confusión
print(conf_matrix_optimal)

# Calcular la precisión del modelo
accuracy_optimal <- sum(diag(conf_matrix_optimal)) / sum(conf_matrix_optimal)
print(paste("Precisión del modelo:", accuracy_optimal))

# Si deseas calcular medidas adicionales como la precisión, recall y F1-score por clase,
# puedes usar la librería caret para obtener un resumen más detallado
# Asegurarte de que tanto las predicciones como los datos reales son factores y que tienen los mismos niveles
levels(predictions_nb_optimal) <- union(levels(predictions_nb_optimal), levels(test_data$consumoCat))
levels(test_data$consumoCat) <- levels(predictions_nb_optimal)

# Generar la matriz de confusión utilizando la librería caret
confusion_matrix <- confusionMatrix(as.factor(predictions_nb_optimal), as.factor(test_data$consumoCat))
print(confusion_matrix)


```
Tras utilizar los valores sugeridos por el tunning, pse observa como la precisión aumenta a un *75.56%*, la cual es la mayor acurracy obtenida hasta este momento, utilizando Naive Bayes. Es un buen porcentaje, pero sigue sin superar otros como el de árboles de decisión.Podemos ver como las sensbilidades tienen valores decentes, siendo estos  0.9667   0.5806   0.8068 para las baratas, estándar y caras. Como podemos observar,clase estándar sigue siendo la más difícil de clasificar lo que sugiere que un análisis más produndo sobre esta categoría o sobre la clasificación. En conclusión, aún con tunning sobre los modelos creados, nos dimos cuenta que Niave Bayes para este caso no es el modelos más óptimo, por lo que debería descartarse como modelo para clasificación en este proyecto.