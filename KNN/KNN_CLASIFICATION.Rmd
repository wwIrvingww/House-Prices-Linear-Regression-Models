---
title: "KNN_clasification"
author: "Irving"
date: "2025-03-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

```{r cargando paquetes, message=FALSE, warning=FALSE}
library(GGally)
library(dplyr)
library(tidyverse)
library(class)
library(ggplot2)
library(caret)
library(MLmetrics)
```


```{r load_data, include=FALSE}
#train data ya tiene el 70 y test_data el 30%
train_data <- read.csv("train_final.csv")
test_data <- read.csv("test_final.csv")
```

```{r remove_na_columns, include=FALSE}
pct_na <- sapply(train_data, function(x) mean(is.na(x))*100)
drop_cols <- names(pct_na[pct_na > 75])
train_data <- train_data %>% select(-all_of(drop_cols))
test_data  <- test_data  %>% select(-all_of(drop_cols))
```

```{r discretization, include=FALSE}
breaks <- quantile(train_data$SalePrice, probs = c(0,1/3,2/3,1), na.rm=TRUE)
labels <- c("barata","estandar","cara")

train_data$CategoriaPrecio <- cut(train_data$SalePrice, breaks=breaks, labels=labels, include.lowest=TRUE)
test_data$CategoriaPrecio  <- cut(test_data$SalePrice,  breaks=breaks, labels=labels, include.lowest=TRUE)

```


```{r process_data, echo=FALSE}

numeric_cols <- train_data %>% select(where(is.numeric)) %>% names()

all_cat <- train_data %>% select(where(~!is.numeric(.))) %>% names()
valid_cat <- all_cat[
  sapply(all_cat, function(col) n_distinct(train_data[[col]]) > 1 && n_distinct(test_data[[col]]) > 1)
]

preNum <- preProcess(train_data[numeric_cols], method="medianImpute")
train_data[numeric_cols] <- predict(preNum, train_data[numeric_cols])
test_data[numeric_cols]  <- predict(preNum, test_data[numeric_cols])

encoder <- dummyVars(~ ., data=train_data[valid_cat], fullRank=TRUE)
train_cat <- predict(encoder, train_data[valid_cat]) %>% as.data.frame()
test_cat  <- predict(encoder, test_data[valid_cat])  %>% as.data.frame()

missing <- setdiff(names(train_cat), names(test_cat))
test_cat[missing] <- 0
test_cat <- test_cat[names(train_cat)]

train_encoded <- bind_cols(train_data[numeric_cols], train_cat, CategoriaPrecio=train_data$CategoriaPrecio)
test_encoded  <- bind_cols(test_data[numeric_cols],  test_cat,  CategoriaPrecio=test_data$CategoriaPrecio)

```


```{r scaling, echo=FALSE}
predictors <- select(train_encoded, -CategoriaPrecio)
nzv <- nearZeroVar(predictors)
if(length(nzv) > 0) {
  predictors <- predictors[ , -nzv]
  test_predictors <- select(test_encoded, -CategoriaPrecio)[ , -nzv]
} else {
  test_predictors <- select(test_encoded, -CategoriaPrecio)
}

preProc <- preProcess(predictors, method = c("center","scale"))

train_scaled <- predict(preProc, predictors)
test_scaled  <- predict(preProc, test_predictors)

train_scaled[is.na(train_scaled)] <- 0
test_scaled[is.na(test_scaled)]   <- 0

response <- train_encoded$CategoriaPrecio

stopifnot(sum(is.na(train_scaled)) == 0, sum(is.na(test_scaled)) == 0)
```

4. Haga un modelo de clasificación, use la variable categórica que hizo con el precio de las casas (barata, media y cara) como variable respuesta.   
 
```{r knn_model, echo=FALSE}
set.seed(123)
knn_pred <- knn(train = train_scaled, test = test_scaled, cl = response, k = 10)
test_encoded$Pred_KNN <- knn_pred

cat("Distribución de clases (reales):\n")
print(table(test_encoded$CategoriaPrecio))
cat("\nDistribución de clases (predichas):\n")
print(table(test_encoded$Pred_KNN))

```
5. Utilice los modelos con el conjunto de prueba y determine la eficiencia del algoritmo para predecir y clasificar.
6. Haga un análisis de la eficiencia del modelo de clasificación usando una matriz de confusión. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores. 
```{r test_model, echo=FALSE}
cm <- confusionMatrix(test_encoded$Pred_KNN, test_encoded$CategoriaPrecio)
print(cm)
cat("Accuracy:", cm$overall["Accuracy"], "\n")

```
7. Analice el modelo. ¿Cree que pueda estar sobreajustado? 
```{r overfitting_check, echo=FALSE}
train_pred <- knn(train = train_scaled, test = train_scaled, cl = response, k = 10)
train_encoded$Pred_KNN <- train_pred

train_cm <- confusionMatrix(train_pred, train_encoded$CategoriaPrecio)
test_cm  <- confusionMatrix(test_encoded$Pred_KNN, test_encoded$CategoriaPrecio)

cat("Train performance\n")
print(train_cm$overall[c("Accuracy","Kappa")])
cat("\nTest performance\n")
print(test_cm$overall[c("Accuracy","Kappa")])

```

8.Haga un modelo usando validación cruzada, compare los resultados de este con los del modelo anterior. ¿Cuál funcionó mejor?

```{r cross_validation_categorical}
# Definir el control de la validación cruzada
ctrl <- trainControl(method = "cv",  # cross val
                     number = 10,    
                     classProbs = TRUE,
                     summaryFunction = multiClassSummary) 

# Definir la grilla de valores para k (número de vecinos)
knn_grid <- expand.grid(k = seq(3, 21, by = 2))  

# Entrenar el modelo con validación cruzada
set.seed(123)
knn_cv_model <- train(x = train_scaled, 
                      y = response, 
                      method = "knn", 
                      trControl = ctrl, 
                      tuneGrid = knn_grid, 
                      metric = "Accuracy")  # Métrica a optimizar


print(knn_cv_model)

knn_cv_pred <- predict(knn_cv_model, newdata = test_scaled)

cm_cv <- confusionMatrix(knn_cv_pred, test_encoded$CategoriaPrecio)
print(cm_cv)
cat("Accuracy (CV):", cm_cv$overall["Accuracy"], "\n")

train_cv_pred <- predict(knn_cv_model, newdata = train_scaled)
train_cm_cv <- confusionMatrix(train_cv_pred, train_encoded$CategoriaPrecio)

cat("Train performance (CV):\n")
print(train_cm_cv$overall[c("Accuracy","Kappa")])
cat("\nTest performance (CV):\n")
print(cm_cv$overall[c("Accuracy","Kappa")])
```

Los resultados fueron menos precisos por tan solo pot aproximadamente un 1%. El módelo sigue siendo bastante bueno para predecir con las clasificaciones, mostrando un accuracy de 0.8647. Además de obtener estos resultados para el set de datos de testing, también lo hicimos con el set de prueba. Esto con el fin de identificar si el modelo realmente es bueno o si este tiene overfitting. COmo podemos ver, realmente los resultados no cambian signficativamente, lo cuál es un buen indicador de que el modelo esta generalizando bien la información y que no está sobre ajustado a datos parecidos a los del entrenamiento. En conclusión los resultados fueron de igual manera satisfactorios, podemos ver que el modelo no se sobreajusta pero tiene un desempeño ligeramente peor. Esto tiene sentido, ya que realmente la validación hace que el modelo sea más robusto y que pueda generalizar mejor, no tanto la mejora del accuracy como tal.

9. Tanto para los modelos de regresión como de clasificación, pruebe con varios valores de los
hiperparámetros ¿Qué parámetros pueden tunearse en un KNN?, use el mejor modelo del
tuneo, ¿Mejoraron los resultados usando el mejor modelo ahora? Explique

```{r tunin}
# Definir la grilla de valores para k
knn_grid <- expand.grid(k = seq(1, 30, by = 1))

# Entrenar el modelo con validación cruzada y pesos ponderados
set.seed(123)
knn_cv_model <- train(x = train_scaled, 
                      y = response, 
                      method = "knn", 
                      trControl = ctrl, 
                      tuneGrid = knn_grid, 
                      metric = "Accuracy",
                      weights = "distance")

# Ver los resultados del tuning
print(knn_cv_model)

# Graficar el rendimiento para diferentes valores de k
ggplot(knn_cv_model) +
  theme_minimal() +
  labs(title = "Rendimiento de KNN para diferentes valores de k",
       x = "Número de vecinos (k)",
       y = "Accuracy")

#ewntrenar el modelo KNN con k = 20
set.seed(123)
knn_model_k20 <- knn(train = train_scaled, 
                     test = test_scaled, 
                     cl = response, 
                     k = 20)


test_encoded$Pred_KNN_k20 <- knn_model_k20


cm_k20 <- confusionMatrix(test_encoded$Pred_KNN_k20, test_encoded$CategoriaPrecio)

print(cm_k20)
cat("Accuracy (k = 20):", cm_k20$overall["Accuracy"], "\n")

```

Aca podemos ver una gran cantidad de resultados distintos. En este caso nos enfocamos principalmente en el tuning de el parametro K, que es la cantidad de vecions k. Para poder tunear el modelo hicimos que se hiciera una entranmiento con una grilla de valores para k. Probamos con valores de k, desde 1 a 30 con un step de 1 para obtener una gráfica que relaciona el número de vecinos con el accuracy luego del entrenamiento con cada uno de los distintos valores. Dentro del tuning se puede ver que los mejores resuiltados se encuentran con un valor k de 20 o 19. Siendo el de 20 el más alto con un accuracy de 0.8627.

10.Compare la eficiencia del algoritmo con el resultado obtenido con el árbol de decisión (el de clasificación), el modelo de random forest y el de naive bayes que hizo en las entregas pasadas. ¿Cuál es mejor para predecir? ¿Cuál se demoró más en procesar?


Por fines prácticos esta comparación se realizará en el documento de la entrega final, ya que los resultados de cada uno de los métodos se encuentran en distintos archivos y bajo largos análisis con múltiples lineas de código que alargarían demasiado este mismo archivo.


